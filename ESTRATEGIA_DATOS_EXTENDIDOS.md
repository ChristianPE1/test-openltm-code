# üìä Estrategia para Dataset Extendido (10-20 A√±os)

**Fecha**: 2025-01-11  
**Dataset**: ERA5 2014-2024 (11 a√±os) ‚Üí Expandible a 2005-2024 (20 a√±os)

---

## üéØ Cambio de Estrategia: 5 a√±os ‚Üí 10-20 a√±os

### ¬øPor qu√© m√°s a√±os es cr√≠tico para ENSO?

**Ciclos ENSO completos**:
- **5 a√±os (2020-2024)**: Solo 1-2 ciclos ENSO completos
  - El Ni√±o 2023-2024 (moderado/fuerte)
  - La Ni√±a 2020-2022 (d√©bil/moderado)
  - Datos insuficientes para patrones complejos

- **11 a√±os (2014-2024)**: 3-4 ciclos ENSO completos ‚úÖ
  - El Ni√±o 2023-2024 (fuerte)
  - La Ni√±a 2020-2022 (moderado)
  - El Ni√±o 2015-2016 (SUPER FUERTE) ‚≠ê **Cr√≠tico**
  - Neutral 2013-2014, 2017-2019

- **20 a√±os (2005-2024)**: 6-8 ciclos ENSO completos ‚úÖ‚úÖ
  - El Ni√±o 2023-2024 (fuerte)
  - La Ni√±a 2020-2022 (moderado)
  - El Ni√±o 2015-2016 (super fuerte)
  - La Ni√±a 2010-2012 (fuerte)
  - El Ni√±o 2009-2010 (moderado)
  - La Ni√±a 2007-2008 (moderado)
  - El Ni√±o 2006-2007 (d√©bil)

**Ventajas de m√°s a√±os**:
1. ‚úÖ **Diversidad de eventos**: Captura El Ni√±o d√©bil, moderado, fuerte y SUPER FUERTE
2. ‚úÖ **Transiciones**: Aprende patrones de transici√≥n La Ni√±a ‚Üí El Ni√±o ‚Üí Neutral
3. ‚úÖ **Robustez**: Reduce overfitting al tener m√°s ejemplos de cada fase
4. ‚úÖ **Eventos raros**: Incluye eventos extremos como El Ni√±o 2015-2016 (devastador en Per√∫)

---

## üî¨ Experimentos Recomendados (Dataset Grande)

### Experimento 1: Transfer Learning vs From Scratch (11 a√±os)

#### Hip√≥tesis
> "Con 11 a√±os de datos, Transfer Learning sigue siendo superior porque el checkpoint ERA5 preentrenado contiene conocimiento general de patrones atmosf√©ricos que acelera convergencia"

#### Configuraci√≥n A: Transfer Learning (Recomendado)
```bash
python run.py \
  --model timer_xl_classifier \
  --seq_len 2880 \      # 120 d√≠as (captura ciclo ENSO completo)
  --e_layers 8 \
  --d_model 1024 \
  --batch_size 12 \     # Reducido por secuencia larga
  --learning_rate 5e-5 \  # M√°s alto que 1e-5 (m√°s datos = m√°s LR)
  --dropout 0.2 \       # Mayor regularizaci√≥n
  --train_epochs 30 \
  --adaptation \
  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth
```

**Expectativa**: F1 = 0.82-0.85 (mejor que 0.79 con 5 a√±os)

#### Configuraci√≥n B: From Scratch (Comparaci√≥n)
```bash
python run.py \
  --model timer_xl_classifier \
  --seq_len 2880 \
  --e_layers 8 \
  --d_model 1024 \
  --batch_size 12 \
  --learning_rate 1e-4 \  # M√°s alto para convergencia desde cero
  --dropout 0.2 \
  --train_epochs 50       # M√°s √©pocas sin pretrain
```

**Expectativa**: F1 = 0.80-0.83 (puede competir con Transfer Learning)

#### An√°lisis Comparativo
| Aspecto | Transfer Learning | From Scratch | Ganador |
|---------|-------------------|--------------|---------|
| **Convergencia** | R√°pida (30 √©pocas) | Lenta (50 √©pocas) | Transfer Learning |
| **F1-Score esperado** | 0.82-0.85 | 0.80-0.83 | Transfer Learning |
| **Tiempo total** | 15-20 horas | 25-35 horas | Transfer Learning |
| **Uso de memoria** | 6 GB | 6 GB | Empate |
| **Robustez** | Alta (conocimiento previo) | Media | Transfer Learning |

**Conclusi√≥n esperada**: Transfer Learning sigue siendo superior, pero la diferencia se reduce con m√°s datos (ŒîF1 ‚âà 0.02-0.03 vs ŒîF1 ‚âà 0.10 con 5 a√±os)

---

### Experimento 2: Small Model Mejorado (11 a√±os)

#### Motivaci√≥n
> "El Small Model original (4 layers, 512 dim) funcion√≥ bien con 5 a√±os (F1=0.78), pero con 11 a√±os puede necesitar m√°s capacidad"

#### Configuraci√≥n Mejorada
```bash
python run.py \
  --model timer_xl_classifier \
  --seq_len 1440 \      # 60 d√≠as (m√°s que 720h = 30 d√≠as original)
  --e_layers 6 \        # M√ÅS capas (antes 4)
  --d_model 768 \       # M√ÅS dimensiones (antes 512)
  --d_ff 1536 \         # Proporcional a d_model
  --batch_size 24 \     # Optimizado para 2-3 GB VRAM
  --learning_rate 8e-5 \
  --dropout 0.15 \      # Mayor regularizaci√≥n
  --train_epochs 25
```

**Mejoras vs versi√≥n original**:
1. ‚úÖ +50% capacidad (6 layers vs 4 layers)
2. ‚úÖ +50% dimensionalidad (768 vs 512)
3. ‚úÖ +100% contexto (1440h vs 720h)
4. ‚úÖ Sigue siendo eficiente (2-3 GB vs 1.5 GB)

**Expectativa**: F1 = 0.80-0.82 (mejora de +2-4% vs F1=0.78 con 5 a√±os)

---

### Experimento 3: An√°lisis de Longitud de Contexto (11 a√±os)

#### Hip√≥tesis
> "Con 11 a√±os de datos, contextos m√°s largos (90-120 d√≠as) capturan mejor los ciclos ENSO que contextos cortos (30-60 d√≠as)"

#### Configuraciones a Probar
```python
context_experiments = [
    {"seq_len": 720,  "dias": 30,  "descripcion": "Corto - Captura variabilidad mensual"},
    {"seq_len": 1440, "dias": 60,  "descripcion": "Medio - Captura 2 meses"},
    {"seq_len": 2160, "dias": 90,  "descripcion": "Largo - M√≠nimo ciclo ENSO (3 meses)"},
    {"seq_len": 2880, "dias": 120, "descripcion": "Muy Largo - Ciclo ENSO completo"},
]
```

**Estrategia de evaluaci√≥n**:
1. Entrenar mismo modelo (8 layers, 1024 dim) con 4 contextos diferentes
2. Medir F1-Score en 3 fases ENSO (El Ni√±o, La Ni√±a, Neutral)
3. Identificar punto de saturaci√≥n (donde ŒîF1 < 0.02 con m√°s contexto)

**Resultados esperados**:
```
Contexto | F1 Global | F1 El Ni√±o | F1 La Ni√±a | F1 Neutral | Tiempo/√âpoca
---------|-----------|------------|------------|------------|-------------
30 d√≠as  | 0.78      | 0.75       | 0.79       | 0.80       | 20 min
60 d√≠as  | 0.82      | 0.80       | 0.83       | 0.83       | 30 min
90 d√≠as  | 0.84      | 0.83       | 0.85       | 0.84       | 40 min
120 d√≠as | 0.85      | 0.84       | 0.86       | 0.85       | 50 min ‚≠ê √ìptimo
```

**Conclusi√≥n esperada**: 90-120 d√≠as es √≥ptimo (captura ciclo ENSO completo sin exceso computacional)

---

## üìà Configuraciones Optimizadas para Dataset Grande

### Transfer Learning Timer-XL (8 layers, 1024 dim)

```python
config_transfer_learning = {
    # Arquitectura
    "e_layers": 8,
    "d_model": 1024,
    "d_ff": 2048,
    "n_heads": 8,
    
    # Contexto (120 d√≠as para capturar ciclo ENSO)
    "seq_len": 2880,  # 120 d√≠as √ó 24 horas
    "input_token_len": 96,
    "output_token_len": 96,
    
    # Optimizaci√≥n
    "batch_size": 12,  # Reducido por secuencia larga
    "learning_rate": 5e-5,  # M√°s alto con m√°s datos
    "dropout": 0.2,  # Mayor regularizaci√≥n
    "train_epochs": 30,
    "patience": 8,
    
    # Transfer Learning
    "adaptation": True,
    "pretrain_model_path": "checkpoints/timer_xl/checkpoint.pth",
    
    # P√©rdida
    "use_focal_loss": True,  # Para desbalance de clases
    "loss": "CE",
    
    # Scheduler
    "cosine": True,
    "tmax": 30
}
```

**Recursos**:
- VRAM: ~6 GB
- Tiempo por √©poca: ~30-45 min (depende de GPU)
- Tiempo total: 15-20 horas (30 √©pocas)

**F1-Score esperado**: 0.82-0.85

---

### Small Model Mejorado (6 layers, 768 dim)

```python
config_small_improved = {
    # Arquitectura (MEJORADA vs versi√≥n original)
    "e_layers": 6,  # Antes: 4
    "d_model": 768,  # Antes: 512
    "d_ff": 1536,  # Antes: 1024
    "n_heads": 8,
    
    # Contexto (60 d√≠as, m√°s que 30 d√≠as original)
    "seq_len": 1440,  # 60 d√≠as √ó 24 horas
    "input_token_len": 96,
    "output_token_len": 96,
    
    # Optimizaci√≥n
    "batch_size": 24,  # Optimizado para 2-3 GB VRAM
    "learning_rate": 8e-5,  # Adaptativo
    "dropout": 0.15,  # Mayor regularizaci√≥n
    "train_epochs": 25,
    "patience": 8,
    
    # P√©rdida
    "use_focal_loss": True,
    "loss": "CE",
    
    # Scheduler
    "cosine": True,
    "tmax": 25
}
```

**Recursos**:
- VRAM: ~2-3 GB
- Tiempo por √©poca: ~15-20 min
- Tiempo total: 6-8 horas (25 √©pocas)

**F1-Score esperado**: 0.80-0.82

---

## üéØ Plan de Ejecuci√≥n (11 A√±os de Datos)

### Fase 1: Preprocesamiento (1-2 horas)
```bash
# Descargar datos ERA5 2014-2024 (11 a√±os)
python preprocessing/preprocess_era5_peru.py \
    --input_dir datasets/raw_era5 \
    --output_dir datasets/processed \
    --years 2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024 \
    --target_horizon 24 \
    --threshold 0.0001  # 0.1 mm
```

**Datos esperados**:
- Total timesteps: ~96,360 (11 a√±os √ó 365 d√≠as √ó 24 horas)
- Total samples (con seq_len=2880): ~93,480
- Distribuci√≥n ENSO: ~40% Neutral, ~30% La Ni√±a, ~30% El Ni√±o

### Fase 2: Entrenamiento Transfer Learning (15-20 horas)
```bash
python run.py \
  --task_name classification \
  --model timer_xl_classifier \
  # ... configuraci√≥n Transfer Learning arriba ...
  --des 'Peru_Rainfall_Transfer_Learning_11Years_2014_2024'
```

**Monitoreo**:
- Guardar checkpoint cada √©poca
- Backup a Drive cada 5 √©pocas (usar celda de backup)
- Early stopping si no mejora por 8 √©pocas

**Expectativa**: F1 > 0.82

### Fase 3: Entrenamiento Small Model Mejorado (6-8 horas)
```bash
python run.py \
  --task_name classification \
  --model timer_xl_classifier \
  # ... configuraci√≥n Small Model Mejorado arriba ...
  --des 'Peru_Rainfall_Small_Improved_11Years_2014_2024'
```

**Expectativa**: F1 > 0.80

### Fase 4: Comparaci√≥n Transfer Learning vs From Scratch (Opcional, 25-35 horas)
```bash
# Solo si Transfer Learning NO supera F1 = 0.83
python run.py \
  --task_name classification \
  --model timer_xl_classifier \
  --seq_len 2880 \
  --e_layers 8 \
  --d_model 1024 \
  --batch_size 12 \
  --learning_rate 1e-4 \  # Sin pretrain necesita m√°s LR
  --train_epochs 50 \     # M√°s √©pocas
  --des 'Peru_Rainfall_From_Scratch_11Years_2014_2024'
```

### Fase 5: Validaci√≥n ENSO (2-3 horas)
```python
# Etiquetar datos por fase ENSO
python scripts/label_enso_phases.py \
    --oni_file data/oni_index_2014_2024.csv \
    --data_file datasets/processed/peru_rainfall_cleaned.csv

# Evaluar por fase
python scripts/evaluate_by_enso_phase.py \
    --checkpoint checkpoints/.../checkpoint.pth \
    --enso_phases data/enso_labeled_data.csv
```

**M√©tricas objetivo**:
- F1 El Ni√±o > 0.80
- F1 La Ni√±a > 0.82
- F1 Neutral > 0.83
- Consistencia: |ŒîF1| < 0.08

---

## üìä Resultados Esperados (11 A√±os vs 5 A√±os)

### Comparaci√≥n Transfer Learning

| M√©trica | 5 A√±os (2020-2024) | 11 A√±os (2014-2024) | Mejora |
|---------|-------------------|---------------------|--------|
| **F1-Score Global** | 0.79 | **0.82-0.85** | +3-6% ‚úÖ |
| **Precision** | 0.71 | **0.78-0.82** | +7-11% ‚úÖ |
| **Recall** | 0.89 | **0.86-0.88** | -1-3% (aceptable) |
| **Accuracy** | 72.87% | **78-82%** | +5-9% ‚úÖ |
| **False Positives** | 49% | **30-35%** | -14-19% ‚úÖ |
| **F1 El Ni√±o** | ~0.76* | **0.80-0.83** | +4-7% ‚úÖ |
| **F1 La Ni√±a** | ~0.80* | **0.82-0.85** | +2-5% ‚úÖ |
| **F1 Neutral** | ~0.80* | **0.83-0.86** | +3-6% ‚úÖ |

*Valores estimados, no medidos directamente en 5 a√±os

### Comparaci√≥n Small Model

| M√©trica | 5 A√±os (4L, 512D) | 11 A√±os (6L, 768D) | Mejora |
|---------|-------------------|-------------------|--------|
| **F1-Score** | 0.78 | **0.80-0.82** | +2-4% ‚úÖ |
| **Precision** | 0.87 | **0.84-0.86** | -1-3% (trade-off) |
| **Recall** | 0.70 | **0.76-0.78** | +6-8% ‚úÖ |
| **Accuracy** | 76.93% | **78-80%** | +1-3% ‚úÖ |

---

## üéØ Decisiones Estrat√©gicas Basadas en Resultados (11 A√±os)

### Escenario 1: Transfer Learning F1 > 0.83 ‚úÖ

**Interpretaci√≥n**: Transfer Learning + m√°s datos es la mejor estrategia

**Pr√≥ximo paso**: Mejoras arquitect√≥nicas
1. ENSO-aware TimeAttention
2. M√°scara Kronecker adaptativa
3. Multi-scale temporal features

**Objetivo**: F1 > 0.87

---

### Escenario 2: Transfer Learning F1 = 0.80-0.83 (Zona intermedia)

**Interpretaci√≥n**: Transfer Learning funciona pero puede mejorar

**Pr√≥ximo paso (Opci√≥n A)**: Probar From Scratch
- Si From Scratch > Transfer Learning: Usar From Scratch de base
- Si Transfer Learning ‚â• From Scratch: Continuar con Transfer Learning

**Pr√≥ximo paso (Opci√≥n B)**: Optimizaci√≥n Transfer Learning
- Entrenar 15-20 √©pocas adicionales
- Ajustar class weights
- Fine-tuning learning rate

---

### Escenario 3: Small Model F1 ‚â• Transfer Learning F1

**Interpretaci√≥n**: Modelo m√°s peque√±o es suficiente (sorprendente pero posible)

**Pr√≥ximo paso**: Usar Small Model como baseline
- M√°s r√°pido para experimentos
- Permite probar m√°s configuraciones
- Menos recursos computacionales

---

## üí° Recomendaciones Pr√°cticas

### 1. Gesti√≥n de Recursos en Colab

**Transfer Learning (6 GB VRAM)**:
- ‚úÖ Google Colab Pro (15 GB VRAM disponible)
- ‚úÖ Google Colab Gratis (12 GB VRAM, ajustado)
- ‚ö†Ô∏è Reducir batch_size si OOM: 12 ‚Üí 8 ‚Üí 6

**Small Model (2-3 GB VRAM)**:
- ‚úÖ Google Colab Gratis
- ‚úÖ Cualquier GPU moderna

### 2. Backup Estrat√©gico

**Cada 5 √©pocas**: Ejecutar celda de backup
```python
# Autom√°tico cada 5 √©pocas
if epoch % 5 == 0:
    !python backup_checkpoint.py
```

**Antes de desconectar**: SIEMPRE ejecutar celda final de backup

### 3. Monitoreo de Convergencia

**Se√±ales de buen entrenamiento**:
- ‚úÖ Train loss decrece consistentemente
- ‚úÖ Val loss decrece (puede estancarse despu√©s de √©poca 15-20)
- ‚úÖ F1-Score sube en val cada 2-3 √©pocas
- ‚úÖ Early stopping act√∫a despu√©s de √©poca 20+

**Se√±ales de problemas**:
- ‚ùå Train loss crece o se estanca desde inicio
- ‚ùå Val loss sube consistentemente (overfitting)
- ‚ùå F1-Score no supera 0.70 despu√©s de 10 √©pocas
- ‚ùå Early stopping act√∫a antes de √©poca 10

---

## üöÄ Siguiente Ejecuci√≥n (Inmediata)

### Paso 1: Descargar datos 2014-2024 (1-2 horas)

```bash
# Descargar 11 a√±os de ERA5
# Usar CDS API o manual download
```

### Paso 2: Preprocesar (30 min)

```bash
python preprocessing/preprocess_era5_peru.py \
    --years 2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024
```

### Paso 3: Entrenar Transfer Learning (15-20 horas)

**Usar configuraci√≥n optimizada en celda del notebook**

**Monitoreo**:
- Backup cada 5 √©pocas
- Detener manualmente si F1 > 0.83 en √©poca 20-25 (ya es excelente)

### Paso 4: Entrenar Small Model Mejorado (6-8 horas)

**Comparar con Transfer Learning**

### Paso 5: Decidir siguiente paso seg√∫n resultados

---

**√öltima Actualizaci√≥n**: 2025-01-11  
**Pr√≥xima Ejecuci√≥n**: Dataset 2014-2024 (11 a√±os)  
**Meta**: F1-Score > 0.82 (Transfer Learning) y > 0.80 (Small Model)
