# üöÄ COMANDO DE ENTRENAMIENTO - GOOGLE COLAB

## ‚úÖ Comando Completo (Copiar y Pegar)

```python
# Entrenar Timer-XL con Transfer Learning
!python run.py \
  --task_name classification \
  --is_training 1 \
  --model_id peru_rainfall_timerxl \
  --model timer_xl_classifier \
  --data PeruRainfall \
  --root_path datasets/processed/ \
  --data_path peru_rainfall.csv \
  --checkpoints checkpoints/ \
  --seq_len 1440 \
  --input_token_len 96 \
  --output_token_len 96 \
  --test_seq_len 1440 \
  --test_pred_len 2 \
  --e_layers 8 \
  --d_model 1024 \
  --d_ff 2048 \
  --n_heads 8 \
  --dropout 0.1 \
  --activation relu \
  --batch_size 128 \
  --learning_rate 1e-5 \
  --train_epochs 50 \
  --patience 10 \
  --n_classes 2 \
  --gpu 0 \
  --cosine \
  --tmax 50 \
  --use_norm \
  --adaptation \
  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \
  --use_focal_loss \
  --loss CE \
  --itr 1 \
  --des 'Peru_Rainfall_Transfer_Learning'
```

---

## üìã Explicaci√≥n de Argumentos Clave

### **Argumentos OBLIGATORIOS** (causaban el error)
```python
--task_name classification      # Tipo de tarea: clasificaci√≥n (no forecast)
--is_training 1                 # Modo entrenamiento (1=train, 0=test)
--model_id peru_rainfall_timerxl # ID √∫nico del experimento
```

### **Argumentos del Modelo**
```python
--model timer_xl_classifier     # Modelo: Timer-XL adaptado a clasificaci√≥n
--data PeruRainfall            # Dataset personalizado
--n_classes 2                  # Clasificaci√≥n binaria (Rain/No Rain)
```

### **Rutas de Datos**
```python
--root_path datasets/processed/           # Carpeta con datos procesados
--data_path peru_rainfall.csv            # Archivo CSV generado por preprocesamiento
--checkpoints checkpoints/               # Carpeta para guardar checkpoints
```

### **Transfer Learning**
```python
--adaptation                                        # Activa transfer learning
--pretrain_model_path checkpoints/timer_xl/checkpoint.pth  # Checkpoint pre-entrenado
```

### **Arquitectura**
```python
--seq_len 1440              # Longitud de secuencia (60 d√≠as √ó 24h)
--input_token_len 96        # Tokens de entrada por ventana
--output_token_len 96       # Tokens de salida
--e_layers 8                # Capas del encoder
--d_model 1024              # Dimensi√≥n del modelo
--d_ff 2048                 # Dimensi√≥n feedforward
--n_heads 8                 # Attention heads
--dropout 0.1               # Dropout rate
```

### **Entrenamiento**
```python
--batch_size 128            # Tama√±o del batch (ajustar si OOM)
--learning_rate 1e-5        # Learning rate (bajo para transfer learning)
--train_epochs 50           # M√°ximo de √©pocas
--patience 10               # Early stopping patience
--cosine                    # Cosine annealing scheduler
--tmax 50                   # T_max para cosine scheduler
```

### **Loss y M√©tricas**
```python
--loss CE                   # Cross-Entropy Loss
--use_focal_loss            # Focal Loss para desbalance de clases
```

### **Otros**
```python
--gpu 0                     # ID de GPU (0 para T4 en Colab)
--use_norm                  # Normalizaci√≥n de datos
--itr 1                     # N√∫mero de iteraciones del experimento
--des 'Peru_Rainfall_Transfer_Learning'  # Descripci√≥n del experimento
```

---

## üîß Ajustes para Optimizar

### **Si obtienes OOM (Out of Memory):**
```python
--batch_size 64    # Reducir a 64 o 32
```

### **Para entrenamiento m√°s r√°pido (menos preciso):**
```python
--train_epochs 20   # Reducir √©pocas
--patience 5        # Reducir patience
```

### **Para mayor precisi√≥n (m√°s lento):**
```python
--train_epochs 100  # Aumentar √©pocas
--learning_rate 5e-6  # Learning rate a√∫n m√°s bajo
```

---

## ‚úÖ Verificaci√≥n Pre-Entrenamiento

**Antes de ejecutar el comando, verifica:**

```python
# 1. GPU disponible
!nvidia-smi

# 2. Checkpoint pre-entrenado existe
!ls -lh checkpoints/timer_xl/checkpoint.pth

# 3. Datos procesados existen
!ls -lh datasets/processed/peru_rainfall.csv

# 4. Estructura de carpetas
!mkdir -p checkpoints/
```

---

## üìä Salida Esperada Durante Entrenamiento

```
Loading pretrained model from checkpoints/timer_xl/checkpoint.pth
‚úÖ Pretrained weights loaded successfully

Epoch 1/50:
  Train Loss: 0.6421
  Valid Loss: 0.5892
  Valid F1: 0.6234
  ‚è±Ô∏è  Time: 15m 30s

Epoch 2/50:
  Train Loss: 0.5234
  Valid Loss: 0.5123
  Valid F1: 0.6890
  ‚è±Ô∏è  Time: 15m 25s

...

Early Stopping at epoch 35
‚úÖ Best model saved to: checkpoints/peru_rainfall_timerxl/checkpoint.pth
üìä Best F1-Score: 0.7245
```

---

## üéØ Despu√©s del Entrenamiento

```python
# Verificar resultados
!ls -lh checkpoints/*/peru_rainfall_timerxl*/

# Copiar a Google Drive
!cp -r checkpoints/*/peru_rainfall_timerxl* /content/drive/MyDrive/timer_xl_peru/results/
```

---

## ‚ùì Soluci√≥n de Problemas

### **Error: "the following arguments are required: --task_name, --is_training, --model_id"**
‚úÖ **Soluci√≥n:** Usar el comando completo de arriba (no omitir argumentos)

### **Error: "FileNotFoundError: checkpoint.pth"**
‚úÖ **Soluci√≥n:** Aseg√∫rate de que el checkpoint est√© en Drive:
```python
!ls -lh /content/drive/MyDrive/timer_xl_peru/checkpoints/checkpoint.pth
```

### **Error: "CUDA out of memory"**
‚úÖ **Soluci√≥n:** Reducir batch_size:
```python
--batch_size 64  # o 32
```

### **Error: "FileNotFoundError: peru_rainfall.csv"**
‚úÖ **Soluci√≥n:** Ejecutar preprocesamiento primero:
```python
!python preprocessing/preprocess_era5_peru.py --years 2022,2023,2024
```

---

## üöÄ Comando TODO-EN-UNO (para Copy-Paste directo)

```bash
# Verificar requisitos
nvidia-smi && \
ls -lh checkpoints/timer_xl/checkpoint.pth && \
ls -lh datasets/processed/peru_rainfall.csv && \

# Entrenar
python run.py \
  --task_name classification \
  --is_training 1 \
  --model_id peru_rainfall_timerxl \
  --model timer_xl_classifier \
  --data PeruRainfall \
  --root_path datasets/processed/ \
  --data_path peru_rainfall.csv \
  --checkpoints checkpoints/ \
  --seq_len 1440 \
  --input_token_len 96 \
  --output_token_len 96 \
  --test_seq_len 1440 \
  --test_pred_len 2 \
  --e_layers 8 \
  --d_model 1024 \
  --d_ff 2048 \
  --n_heads 8 \
  --dropout 0.1 \
  --activation relu \
  --batch_size 128 \
  --learning_rate 1e-5 \
  --train_epochs 50 \
  --patience 10 \
  --n_classes 2 \
  --gpu 0 \
  --cosine \
  --tmax 50 \
  --use_norm \
  --adaptation \
  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \
  --use_focal_loss \
  --loss CE \
  --itr 1 \
  --des 'Peru_Rainfall_Transfer_Learning'

echo "‚úÖ Entrenamiento completado!"
```

---

## üìû Siguiente Paso

**AHORA EN COLAB:**
1. Abre el notebook actualizado
2. Ejecuta todas las celdas hasta "Train Timer-XL"
3. Copia y pega el comando completo de arriba
4. Espera ~4-6 horas

**¬°Listo!** üéâ
