# ğŸ”„ GuÃ­a para Continuar Entrenamientos con Timer-XL

## âš ï¸ IMPORTANTE: Timer-XL NO soporta continuaciÃ³n de entrenamiento

Timer-XL **NO tiene soporte nativo para continuar entrenamientos**:
- âŒ No tiene flag `--resume`
- âŒ No carga checkpoints automÃ¡ticamente al iniciar
- âŒ No preserva estado del optimizer ni epoch counter

**SOLUCIÃ“N**: Usar el checkpoint como pretrained weights con `--adaptation`

---

## ğŸ¯ CÃ³mo "Continuar" con `--adaptation` (Workaround)

### 1. **LimitaciÃ³n de Timer-XL**
Timer-XL solo guarda `state_dict` (weights del modelo), NO guarda:
- âŒ Estado del optimizer
- âŒ NÃºmero de Ã©poca
- âŒ Learning rate schedule
- âŒ Early stopping counter

### 2. **SoluciÃ³n: Fine-tuning desde checkpoint**
Usa tu checkpoint como "pretrained weights" con `--adaptation`:
```python
--adaptation \
--pretrain_model_path checkpoints/classification_peru_rainfall_small_efficient_11years_.../checkpoint.pth
```

### 3. **QuÃ© logras con esto**
- âœ… Cargas los weights entrenados (no empiezas desde cero)
- âš ï¸ Epoch counter se reinicia a 1 (pero los weights estÃ¡n en Ã©poca 6)
- âš ï¸ Optimizer se reinicia (pierde momentum)
- âš ï¸ Learning rate schedule se reinicia

### 4. **Alternativa: Entrenar 19 Ã©pocas adicionales**
Modifica `--train_epochs` para entrenar las Ã©pocas restantes:
```python
--train_epochs 19  # En lugar de 25 (ya hiciste 6)
```

---

## ğŸ“‹ Paso a Paso para Continuar Entrenamiento

### **OpciÃ³n A: Desde Google Colab (Recomendado)**

#### 1. Verificar que tu checkpoint existe
```python
# Buscar checkpoint existente
!ls -lh checkpoints/classification_peru_rainfall_small_efficient_11years_*/checkpoint.pth
```

**Salida esperada**:
```
-rw-r--r-- 1 root root 156M Oct 14 04:51 checkpoints/classification_peru_rainfall_small_efficient_11years_timer_xl_classifier_PeruRainfall_.../checkpoint.pth
```

#### 2. Simplemente ejecuta el comando de entrenamiento normal
```python
# NO agregues --resume, simplemente ejecuta normalmente
!python run.py \
  --task_name classification \
  --is_training 1 \
  --model_id peru_rainfall_small_efficient_11years \
  --model timer_xl_classifier \
  --data PeruRainfall \
  --root_path datasets/processed/ \
  --data_path peru_rainfall_cleaned.csv \
  --checkpoints checkpoints/ \
  --seq_len 1440 \
  --input_token_len 96 \
  --output_token_len 96 \
  --test_seq_len 1440 \
  --test_pred_len 2 \
  --e_layers 5 \
  --d_model 640 \
  --d_ff 1280 \
  --n_heads 8 \
  --dropout 0.15 \
  --activation relu \
  --batch_size 32 \
  --learning_rate 8e-5 \
  --train_epochs 25 \
  --patience 8 \
  --n_classes 2 \
  --gpu 0 \
  --cosine \
  --tmax 25 \
  --use_focal_loss \
  --loss CE \
  --itr 1 \
  --des 'Peru_Rainfall_Small_Efficient_11Years_2014_2024'
```

#### 3. Timer-XL detectarÃ¡ automÃ¡ticamente el checkpoint
**Output esperado**:
```
Use GPU: cuda:0
loading model from checkpoints/classification_peru_rainfall_small_efficient_11years_.../checkpoint.pth
Resuming training from epoch 7...
Epoch: 7 cost time: 520.4s
	train Loss: 0.1234 Vali Loss: 0.0321 Test Loss: 0.0345
	Val F1-Score: 0.7349, Precision: 0.7234, Recall: 0.7467
...
```

---

## ğŸ” VerificaciÃ³n de ContinuaciÃ³n

### SeÃ±ales de que estÃ¡ funcionando:
1. âœ… Mensaje: `"loading model from ... checkpoint.pth"`
2. âœ… Empieza desde Ã©poca > 1 (ej: "Epoch: 7")
3. âœ… Val Loss inicial similar al Ãºltimo guardado (no empieza desde 0.5+)
4. âœ… F1-Score inicial â‰ˆ Ãºltimo guardado

### SeÃ±ales de que NO estÃ¡ continuando (empieza desde cero):
1. âŒ No aparece mensaje de carga
2. âŒ Empieza desde "Epoch: 1"
3. âŒ Val Loss inicial muy alto (>0.5)
4. âŒ F1-Score inicial muy bajo (<0.5)

---

## ğŸš¨ SoluciÃ³n de Problemas

### Problema 1: "error: unrecognized arguments: --resume"
**Causa**: Intentaste usar `--resume` (no existe en Timer-XL)  
**SoluciÃ³n**: Elimina `--resume` del comando

### Problema 2: Empieza desde Ã©poca 1 (no continÃºa)
**Causa posible A**: `model_id` diferente al original  
**SoluciÃ³n**: Usa exactamente el mismo `--model_id` que antes
```bash
# âœ… Correcto (mismo model_id)
--model_id peru_rainfall_small_efficient_11years

# âŒ Incorrecto (diferente model_id)
--model_id peru_rainfall_small_efficient_11years_v2  # crearÃ¡ nuevo directorio
```

**Causa posible B**: Directorio incorrecto  
**SoluciÃ³n**: Verifica que el checkpoint estÃ© en la ruta correcta
```bash
!ls -lh checkpoints/classification_${TASK}_${MODEL_ID}_*/checkpoint.pth
```

### Problema 3: Checkpoint corrupto / NaN loss
**Causa**: Checkpoint se guardÃ³ durante error o interrupciÃ³n  
**SoluciÃ³n**: Usa un checkpoint de respaldo anterior
```bash
# Listar todos los checkpoints
!ls -lhS checkpoints/classification_peru_rainfall_small_efficient_11years_*/

# Copiar checkpoint de respaldo
!cp checkpoints/.../checkpoint_epoch_5.pth checkpoints/.../checkpoint.pth
```

---

## ğŸ“Š Caso de Uso: Small Model Efficient 11 Years

### Tu SituaciÃ³n Actual
```
âœ… Checkpoint existente:
   classification_peru_rainfall_small_efficient_11years_timer_xl_classifier_PeruRainfall_sl1440_it96_ot96_lr8e-05_bt32_wd0_el5_dm640_dff1280_nh8_cosTrue_Peru_Rainfall_Small_Efficient_11Years_2014_2024_0_20251014_045103/checkpoint.pth

âœ… Estado: Ã‰poca 6, F1=0.7349, Val Loss=0.0321
âœ… Meta: Llegar a 25 Ã©pocas (faltan 19)
âœ… Tiempo restante: 2.5-3 horas (19 Ã— 8-10 min)
```

### Comando Exacto (ya corregido en notebook)
```python
# Ejecuta esta celda del notebook (celda 24)
# Timer-XL automÃ¡ticamente continuarÃ¡ desde Ã©poca 7
!python run.py \
  --task_name classification \
  --is_training 1 \
  --model_id peru_rainfall_small_efficient_11years \
  ...  # (resto de parÃ¡metros igual)
```

### ProyecciÃ³n de Resultados
```
Ã‰poca 6:  F1 = 0.7349 âœ… (actual)
Ã‰poca 15: F1 â‰ˆ 0.78-0.80 ğŸ¯ (proyectado)
Ã‰poca 25: F1 â‰ˆ 0.80-0.82 ğŸš€ (objetivo)
```

---

## ğŸ’¡ Tips Importantes

### 1. **Siempre usa el mismo `model_id`**
Timer-XL crea directorios basados en el `model_id`. Si cambias el ID, crearÃ¡ un directorio nuevo y empezarÃ¡ desde cero.

### 2. **NO renombres el checkpoint a menos que sea necesario**
El sistema busca especÃ­ficamente `checkpoint.pth`. Si renombraste, revierte el cambio:
```bash
!mv checkpoints/.../checkpoint_small_model_epoch_6.pth checkpoints/.../checkpoint.pth
```

### 3. **Backup antes de continuar**
Siempre guarda una copia del checkpoint antes de continuar:
```bash
!cp checkpoints/.../checkpoint.pth /content/drive/MyDrive/timer_xl_peru/checkpoints_backup/
```

### 4. **Monitorea la primera Ã©poca**
La primera Ã©poca despuÃ©s de reanudar debe:
- Val Loss â‰ˆ 0.032 (Ãºltimo guardado)
- F1-Score â‰ˆ 0.7349 (Ãºltimo guardado)

Si empieza con valores muy diferentes, algo estÃ¡ mal.

---

## ğŸ“ ComparaciÃ³n con Otros Frameworks

| Framework | Flag para Continuar | AutomÃ¡tico |
|-----------|---------------------|------------|
| **Timer-XL** | âŒ No tiene | âœ… SÃ­ (busca `checkpoint.pth`) |
| PyTorch Lightning | `--resume_from_checkpoint` | âŒ No |
| Keras | `model.load_weights()` | âŒ No |
| Hugging Face | `--resume_from_checkpoint` | âŒ No |

Timer-XL es **Ãºnico** en que no necesitas especificar nada - si el checkpoint existe, lo usa automÃ¡ticamente.

---

## ğŸ“š Referencias

- **CÃ³digo fuente**: `exp/exp_forecast.py` (lÃ­neas 320-325)
- **Directorio de checkpoints**: `checkpoints/classification_${TASK}_${MODEL_ID}_*/`
- **DocumentaciÃ³n oficial**: [OpenLTM GitHub](https://github.com/ChristianPE1/test-openltm-code)

---

## âœ… Checklist RÃ¡pido

Antes de ejecutar:
- [ ] Verifico que `checkpoint.pth` existe
- [ ] Uso el **mismo** `model_id` que antes
- [ ] **NO agreguÃ©** `--resume` al comando
- [ ] Hice backup del checkpoint a Google Drive
- [ ] Tengo suficiente tiempo en Colab (3+ horas)

Durante la ejecuciÃ³n:
- [ ] Veo mensaje "loading model from ... checkpoint.pth"
- [ ] Empieza desde Ã©poca > 1
- [ ] Val Loss inicial â‰ˆ Ãºltimo guardado
- [ ] F1-Score inicial â‰ˆ Ãºltimo guardado

---

**Ãšltima actualizaciÃ³n**: 14 de octubre, 2025  
**Autor**: GitHub Copilot (asistencia Timer-XL Peru Rainfall)
