# ğŸŒ§ï¸ Timer-XL Adaptation for Peru Rainfall Prediction

## ğŸ“‹ Project Overview

This repository adapts **Timer-XL** (Long-Context Transformer) for **binary rainfall prediction** in Peru using ERA5 reanalysis data.

**Goal**: Predict if it will rain in the next 24 hours (RainTomorrow) using long-context atmospheric data.

---

## ğŸš€ Quick Start (Google Colab)

### 1. Clone Repository
```bash
# In Colab cell
!git clone https://github.com/YOUR_USERNAME/AdaptationOpenLTM.git
%cd AdaptationOpenLTM
```

### 2. Install Dependencies
```bash
!pip install -r requirements.txt
```

### 3. Upload ERA5 Data
```bash
# Upload your .zip files to datasets/raw_era5/
# Expected files:
# - era5_peru_2023.zip
# - era5_peru_2024.zip
# (Or download 10 years: 2014-2024)
```

### 4. Preprocess Data
```python
!python preprocessing/preprocess_era5_peru.py \
    --input_dir datasets/raw_era5 \
    --output_dir datasets/processed \
    --years 2023,2024 \
    --target_horizon 24h
```

### 5. Run Transfer Learning
```bash
!bash scripts/adaptation/peru_rainfall/train_timerxl_peru.sh
```

---

## ğŸ“Š Data Requirements

### **Recommended: Start with 2-3 Years for Testing**
- **Minimum**: 2 aÃ±os (2023-2024) â†’ ~17,520 timesteps (12h resolution)
- **Recommended for testing**: 3 aÃ±os (2022-2024) â†’ ~26,280 timesteps
- **Optimal for final model**: 10 aÃ±os (2014-2024) â†’ ~87,600 timesteps

### **Why Start Small?**
âœ… Faster preprocessing (~10 min vs 1 hour)
âœ… Faster training (2-3 hours vs 10-15 hours on T4)
âœ… Quick iteration to debug pipeline
âœ… Validate methodology before full-scale training

### **Download Strategy**
```python
testing_phase = {
    "data": "2022-2024 (3 years)",
    "purpose": "Debug pipeline, validate approach",
    "training_time": "~3 hours on T4",
    "checkpoint": "Save after validation"
}

full_training = {
    "data": "2014-2024 (10 years)", 
    "purpose": "Final model, publication results",
    "training_time": "~12 hours on T4",
    "after": "Testing phase successful"
}
```

---

## ğŸ“ Directory Structure

```
AdaptationOpenLTM/
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ raw_era5/                   # â† UPLOAD .zip FILES HERE
â”‚   â”‚   â”œâ”€â”€ era5_peru_2022.zip     # Manual upload or script download
â”‚   â”‚   â”œâ”€â”€ era5_peru_2023.zip
â”‚   â”‚   â””â”€â”€ era5_peru_2024.zip
â”‚   â”‚
â”‚   â””â”€â”€ processed/                  # Generated by preprocessing
â”‚       â”œâ”€â”€ peru_rainfall_train.csv
â”‚       â”œâ”€â”€ peru_rainfall_val.csv
â”‚       â”œâ”€â”€ peru_rainfall_test.csv
â”‚       â””â”€â”€ preprocessing_stats.json
â”‚
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ preprocess_era5_peru.py     # Main preprocessing script
â”‚   â”œâ”€â”€ feature_engineering.py      # Feature creation
â”‚   â””â”€â”€ download_era5.py            # CDS API downloader (optional)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ timer_xl_classifier.py      # Modified Timer-XL for classification
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ adaptation/
â”‚       â””â”€â”€ peru_rainfall/
â”‚           â”œâ”€â”€ train_timerxl_peru.sh
â”‚           â””â”€â”€ eval_timerxl_peru.sh
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb   # EDA
â”‚   â”œâ”€â”€ 02_preprocessing_demo.ipynb # Preprocessing demo
â”‚   â””â”€â”€ 03_training_demo.ipynb      # Training demo
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ timer_xl/
â”‚       â””â”€â”€ checkpoint.pth          # Pre-trained weights (260B time points)
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ peru_rainfall/              # Training outputs
â”‚
â””â”€â”€ README_PERU_RAINFALL.md         # This file
```

---

## ğŸ”§ Configuration

### ERA5 Variables Used
```python
variables = [
    'total_precipitation',           # Target variable
    'temperature_2m',                # Temperature at 2m
    'dewpoint_2m',                   # Dew point temperature
    'surface_pressure',              # Surface pressure
    'mean_sea_level_pressure',       # MSL pressure
    'u_component_of_wind_10m',       # U-wind
    'v_component_of_wind_10m',       # V-wind
    'relative_humidity',             # Relative humidity
    'total_column_water_vapour',     # TCWV
    'convective_available_potential_energy'  # CAPE
]
```

### Regions (Spatial Aggregation)
```python
regions = {
    'costa_norte': {'lat': (0, -5), 'lon': (-82, -78)},    # Tumbes, Piura
    'costa_centro': {'lat': (-5, -10), 'lon': (-82, -76)}, # Lambayeque, La Libertad
    'costa_sur': {'lat': (-10, -18), 'lon': (-80, -70)},   # Lima, Ica, Arequipa
    'sierra_norte': {'lat': (0, -10), 'lon': (-78, -73)},  # Northern Andes
    'sierra_sur': {'lat': (-10, -18), 'lon': (-76, -68)}   # Southern Andes
}
```

### Model Configuration
```python
model_config = {
    # Timer-XL Architecture
    'd_model': 1024,
    'n_heads': 8,
    'e_layers': 8,
    'd_ff': 2048,
    'dropout': 0.1,
    
    # Context Windows (for experiments)
    'context_lengths': [
        180,   # 90 days (12h resolution: 90*2=180)
        360,   # 180 days
        730,   # 1 year
        1460,  # 2 years
        2190   # 3 years
    ],
    
    # Prediction
    'prediction_horizon': 2,  # 24 hours ahead (12h resolution: 24/12=2)
    'target': 'binary_rainfall',  # 0: No Rain, 1: Rain
    'threshold': 0.1  # mm (definition of "rain")
}
```

---

## ğŸ¯ Training Strategy

### Transfer Learning Setup
```python
training = {
    'strategy': 'full_shot_fine_tuning',
    'pretrained_weights': 'checkpoints/timer_xl/checkpoint.pth',
    'freeze_encoder': False,  # Fine-tune all layers
    
    'hyperparameters': {
        'learning_rate': 1e-5,    # Low LR for pre-trained weights
        'batch_size': 256,         # T4 GPU: 16GB VRAM
        'epochs': 50,
        'patience': 10,            # Early stopping
        'optimizer': 'AdamW',
        'scheduler': 'CosineAnnealingLR'
    },
    
    'loss_function': {
        'type': 'FocalLoss',       # Handle class imbalance
        'alpha': 0.25,
        'gamma': 2.0
    }
}
```

### Data Splits
```python
splits = {
    'train': '2022-01 to 2023-12',  # 70% (~2 years)
    'val':   '2024-01 to 2024-06',  # 15% (~6 months)
    'test':  '2024-07 to 2024-12'   # 15% (~6 months)
}
```

---

## ğŸ“Š Expected Performance

### Target Metrics (Testing Phase: 3 years)
```python
expected_metrics = {
    'minimum_acceptable': {
        'F1': 0.65,
        'AUC': 0.75,
        'Recall': 0.60
    },
    'desirable': {
        'F1': 0.70,
        'AUC': 0.80,
        'Recall': 0.70
    }
}
```

### Target Metrics (Full Training: 10 years)
```python
final_metrics = {
    'minimum_acceptable': {
        'F1': 0.75,
        'AUC': 0.85,
        'Recall': 0.70
    },
    'desirable': {
        'F1': 0.80,
        'AUC': 0.90,
        'Recall': 0.80
    }
}
```

---

## ğŸ§ª Experiments

### 1. Context Length Ablation
```bash
# Test different lookback windows
for context in 180 360 730 1460 2190; do
    python run.py --seq_len $context --model_id "context_${context}"
done
```

### 2. ENSO Phase Analysis
```python
# Evaluate by climate phase
phases = {
    'El NiÃ±o': ['2023-03 to 2023-09'],  # Example period
    'La NiÃ±a': ['2022-07 to 2023-02'],
    'Neutral': ['2024-01 to 2024-12']
}
```

---

## ğŸ’¾ Google Colab Optimization

### GPU Check
```python
import torch
print(f"GPU Available: {torch.cuda.is_available()}")
print(f"GPU Name: {torch.cuda.get_device_name(0)}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

### Memory Management
```python
# For T4 GPU (16GB VRAM)
optimal_config = {
    'batch_size': 256,         # Adjust if OOM
    'num_workers': 2,          # Colab has 2 CPU cores
    'pin_memory': True,
    'mixed_precision': True    # FP16 training
}
```

### Save Checkpoints to Drive
```python
from google.colab import drive
drive.mount('/content/drive')

# Save to Drive to avoid losing progress
checkpoint_dir = '/content/drive/MyDrive/timer_xl_checkpoints/'
```

---

## ğŸ“š References

- **Timer-XL Paper**: https://arxiv.org/abs/2410.04803
- **OpenLTM Repository**: https://github.com/thuml/OpenLTM
- **ERA5 Documentation**: https://cds.climate.copernicus.eu/

---

## ğŸ†˜ Troubleshooting

### Issue: CUDA Out of Memory
```bash
# Reduce batch size
--batch_size 128  # or 64

# Reduce sequence length
--seq_len 360  # instead of 730
```

### Issue: Slow Preprocessing
```bash
# Use multiprocessing
--num_workers 4

# Process years separately
--years 2023  # one at a time
```

### Issue: Download ERA5 Failed
```bash
# Manual download from CDS:
# https://cds.climate.copernicus.eu/
# Upload .zip to datasets/raw_era5/
```

---

## ğŸ“ Next Steps

1. âœ… Upload 2-3 years of ERA5 data (2022-2024)
2. âœ… Run preprocessing script
3. âœ… Execute training with transfer learning
4. âœ… Analyze results and metrics
5. â­ï¸ If successful, download full 10 years and retrain

---

**Author**: [Your Name]  
**Contact**: [Your Email]  
**Last Updated**: October 2025
