{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb4827",
   "metadata": {},
   "source": [
    "# ðŸŒ§ï¸ Timer-XL Peru Rainfall Prediction - Google Colab\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training Timer-XL on Peru rainfall data.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment\n",
    "2. Upload ERA5 data\n",
    "3. Preprocess data\n",
    "4. Train Timer-XL with transfer learning\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb58468",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660775a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ChristianPE1/test-openltm-code.git\n",
    "%cd test-openltm-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b831b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92910c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (to download checkpoint.pth and save training results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524612e",
   "metadata": {},
   "source": [
    "## 2. Verificar Datos ERA5\n",
    "\n",
    "**Los archivos .nc ya estÃ¡n en el repositorio** (datasets/raw_era5/)  \n",
    "Solo necesitas verificar que se clonaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ERA5 files are in the repository\n",
    "!ls -lh datasets/raw_era5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9543ac7",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing script\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold 0.1\n",
    "\n",
    "print(\"\\nâœ… Preprocessing complete!\")\n",
    "print(\"ðŸ“Š Output files saved to: datasets/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for quick inspection\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Load statistics\n",
    "with open('datasets/processed/preprocessing_stats.json') as f:\n",
    "    stats = json.load(f)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a55ac",
   "metadata": {},
   "source": [
    "## ðŸš¨ CRITICAL: Verify Class Balance\n",
    "\n",
    "**Before training, we MUST check that both classes exist!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Check class distribution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "\n",
    "print(\"ðŸ“Š Class Distribution Analysis:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   rain_24h column:\")\n",
    "print(df['rain_24h'].value_counts())\n",
    "print(f\"\\n   Percentage:\")\n",
    "print(df['rain_24h'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check precipitation values\n",
    "print(f\"\\nðŸŒ§ï¸ Precipitation Statistics:\")\n",
    "print(f\"   Min: {df['precipitation'].min():.4f} mm\")\n",
    "print(f\"   Max: {df['precipitation'].max():.4f} mm\")\n",
    "print(f\"   Mean: {df['precipitation'].mean():.4f} mm\")\n",
    "print(f\"   Median: {df['precipitation'].median():.4f} mm\")\n",
    "print(f\"   95th percentile: {df['precipitation'].quantile(0.95):.4f} mm\")\n",
    "\n",
    "# Check if threshold needs adjustment\n",
    "threshold = 0.1  # Current threshold\n",
    "samples_above_threshold = (df['precipitation'] >= threshold).sum()\n",
    "print(f\"\\nâš ï¸  Samples with precipitation >= {threshold} mm: {samples_above_threshold} ({samples_above_threshold/len(df)*100:.2f}%)\")\n",
    "\n",
    "if samples_above_threshold < 100:\n",
    "    print(f\"\\nâŒ PROBLEM DETECTED:\")\n",
    "    print(f\"   Only {samples_above_threshold} rain events found!\")\n",
    "    print(f\"   This is insufficient for training a classifier.\")\n",
    "    print(f\"\\nðŸ’¡ SOLUTION:\")\n",
    "    print(f\"   Need to lower the threshold or check ERA5 data quality\")\n",
    "    suggested_threshold = df['precipitation'].quantile(0.5)\n",
    "    print(f\"   Suggested threshold (50th percentile): {suggested_threshold:.4f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208b900",
   "metadata": {},
   "source": [
    "## ðŸ”§ Optional: Re-preprocess with Adjusted Threshold\n",
    "\n",
    "**Run this ONLY if the class distribution check above shows imbalanced data (< 10% rain events)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run preprocessing with adjusted threshold for better class balance\n",
    "# This creates a more balanced dataset by lowering the rain threshold\n",
    "\n",
    "# Calculate appropriate threshold (aiming for ~30-40% rain events)\n",
    "df_temp = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "suggested_threshold = df_temp['precipitation'].quantile(0.65)  # 35% will be \"rain\"\n",
    "\n",
    "print(f\"ðŸŽ¯ Suggested threshold: {suggested_threshold:.4f} mm\")\n",
    "print(f\"   This should give ~35% rain events\\n\")\n",
    "\n",
    "# Re-run preprocessing\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold {suggested_threshold:.4f}\n",
    "\n",
    "print(\"\\nâœ… Data re-processed with adjusted threshold!\")\n",
    "print(\"ðŸ“Š Now check class distribution again...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6979",
   "metadata": {},
   "source": [
    "## 4. Train Timer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy pre-trained checkpoint from Google Drive\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'checkpoints/timer_xl'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint.pth'\n",
    "\n",
    "\n",
    "!mkdir -p checkpoints/timer_xl/\n",
    "\n",
    "!cp '/content/drive/MyDrive/timer_xl_peru/checkpoints/checkpoint.pth' \\\n",
    "    checkpoints/timer_xl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyTorch memory configuration to reduce fragmentation\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Train Timer-XL with transfer learning\n",
    "# This will take 4-6 hours on T4 GPU\n",
    "\n",
    "# NOTE: If you get OOM (Out of Memory) error, reduce batch_size:\n",
    "# --batch_size 16  (for T4 with 16GB VRAM - current setting)\n",
    "# --batch_size 8   (if still OOM)\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_timerxl \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.1 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --train_epochs 50 \\\n",
    "  --patience 10 \\\n",
    "  --use_norm \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 50 \\\n",
    "  --adaptation \\\n",
    "  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Transfer_Learning'\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(\"ðŸ“Š Results saved to: checkpoints/peru_rainfall_timerxl/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae69d19",
   "metadata": {},
   "source": [
    "## 5. Save Checkpoint to Drive\n",
    "\n",
    "Prevent losing your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63515cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy training results to Google Drive (prevent losing trained model!)\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Find the checkpoint directory\n",
    "checkpoint_base = 'checkpoints'\n",
    "results_pattern = f'{checkpoint_base}/*/peru_rainfall_timerxl*/'\n",
    "\n",
    "matching_dirs = glob.glob(results_pattern)\n",
    "\n",
    "if matching_dirs:\n",
    "    results_path = matching_dirs[0]\n",
    "    \n",
    "    # Copy entire results folder to Drive\n",
    "    drive_results = '/content/drive/MyDrive/timer_xl_peru/results/'\n",
    "    os.makedirs(drive_results, exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ’¾ Copying results to Google Drive...\")\n",
    "    print(f\"   From: {results_path}\")\n",
    "    print(f\"   To: {drive_results}\")\n",
    "    \n",
    "    # Use shutil for better error handling\n",
    "    try:\n",
    "        shutil.copytree(results_path, os.path.join(drive_results, os.path.basename(results_path.rstrip('/'))), dirs_exist_ok=True)\n",
    "        print(\"âœ… Checkpoint and results saved to Google Drive!\")\n",
    "        print(f\"ðŸ“ Location: {drive_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error copying to Drive: {e}\")\n",
    "        print(\"   You can manually copy from:\", results_path)\n",
    "else:\n",
    "    print(\"âš ï¸ No results found. Training may have failed or is still in progress.\")\n",
    "    print(\"   Expected pattern:\", results_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370af7c",
   "metadata": {},
   "source": [
    "## 6. Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display test results\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Find results directory\n",
    "checkpoint_base = 'checkpoints'\n",
    "results_pattern = f'{checkpoint_base}/*/peru_rainfall_timerxl*/'\n",
    "matching_dirs = glob.glob(results_pattern)\n",
    "\n",
    "if matching_dirs:\n",
    "    results_dir = matching_dirs[0]\n",
    "    print(f\"ðŸ“‚ Results directory: {results_dir}\\n\")\n",
    "    \n",
    "    # List all files\n",
    "    print(\"ðŸ“„ Files in results:\")\n",
    "    for file in os.listdir(results_dir):\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # Try to load metrics\n",
    "    metrics_files = glob.glob(os.path.join(results_dir, '*metrics*.json'))\n",
    "    \n",
    "    if metrics_files:\n",
    "        print(f\"\\nðŸ“Š Loading metrics from: {metrics_files[0]}\")\n",
    "        with open(metrics_files[0]) as f:\n",
    "            metrics = json.load(f)\n",
    "        print(\"\\nâœ… Test Metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No metrics file found yet. Training may still be in progress.\")\n",
    "else:\n",
    "    print(\"âš ï¸ No results directory found. Training may have failed.\")\n",
    "    print(f\"   Expected pattern: {results_pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268598ae",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "**Next steps:**\n",
    "1. Download results from `results/peru_rainfall/`\n",
    "2. Analyze confusion matrix and classification report\n",
    "3. Try different context lengths (seq_len)\n",
    "4. Experiment with different hyperparameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
