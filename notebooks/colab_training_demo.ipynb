{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb4827",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Timer-XL Peru Rainfall Prediction - Google Colab\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training Timer-XL on Peru rainfall data.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment\n",
    "2. Upload ERA5 data\n",
    "3. Preprocess data\n",
    "4. Train Timer-XL with transfer learning\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb58468",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660775a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ChristianPE1/test-openltm-code.git\n",
    "%cd test-openltm-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b831b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92910c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (to download checkpoint.pth and save training results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524612e",
   "metadata": {},
   "source": [
    "## 2. Verificar Datos ERA5\n",
    "\n",
    "**Los archivos .nc ya est√°n en el repositorio** (datasets/raw_era5/)  \n",
    "Solo necesitas verificar que se clonaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ERA5 files are in the repository\n",
    "!ls -lh datasets/raw_era5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9543ac7",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing script\n",
    "# ‚ö†Ô∏è IMPORTANT: ERA5 precipitation is in METERS, not millimeters!\n",
    "# Use threshold in METERS: 0.1 mm = 0.0001 m\n",
    "\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold 0.0001\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(\"üìä Output files saved to: datasets/processed/\")\n",
    "print(\"üí° Threshold: 0.0001 m = 0.1 mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for quick inspection\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Load statistics\n",
    "with open('datasets/processed/preprocessing_stats.json') as f:\n",
    "    stats = json.load(f)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a55ac",
   "metadata": {},
   "source": [
    "## üö® CRITICAL: Verify Class Balance\n",
    "\n",
    "**Before training, we MUST check that both classes exist!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Check class distribution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "\n",
    "print(\"üìä Class Distribution Analysis:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   rain_24h column:\")\n",
    "print(df['rain_24h'].value_counts())\n",
    "print(f\"\\n   Percentage:\")\n",
    "print(df['rain_24h'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check precipitation values\n",
    "print(f\"\\nüåßÔ∏è Precipitation Statistics (in METERS from ERA5):\")\n",
    "print(f\"   Min: {df['precipitation'].min():.6f} m = {df['precipitation'].min()*1000:.3f} mm\")\n",
    "print(f\"   Max: {df['precipitation'].max():.6f} m = {df['precipitation'].max()*1000:.3f} mm\")\n",
    "print(f\"   Mean: {df['precipitation'].mean():.6f} m = {df['precipitation'].mean()*1000:.3f} mm\")\n",
    "print(f\"   Median: {df['precipitation'].median():.6f} m = {df['precipitation'].median()*1000:.3f} mm\")\n",
    "print(f\"   95th percentile: {df['precipitation'].quantile(0.95):.6f} m = {df['precipitation'].quantile(0.95)*1000:.3f} mm\")\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: ERA5 precipitation is in METERS, not millimeters!\n",
    "# Threshold must be in meters too\n",
    "threshold_mm = 0.1  # Target in mm\n",
    "threshold_m = threshold_mm / 1000.0  # Convert to meters\n",
    "\n",
    "samples_above_threshold = (df['precipitation'] >= threshold_m).sum()\n",
    "print(f\"\\n‚ö†Ô∏è  Samples with precipitation >= {threshold_mm} mm ({threshold_m:.6f} m): {samples_above_threshold} ({samples_above_threshold/len(df)*100:.2f}%)\")\n",
    "\n",
    "if samples_above_threshold < len(df) * 0.1:\n",
    "    print(f\"\\n‚ö†Ô∏è Class imbalance detected!\")\n",
    "    print(f\"   Only {samples_above_threshold} rain events ({samples_above_threshold/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\nüí° SOLUTION:\")\n",
    "    # Calculate threshold for 30-35% rain events\n",
    "    suggested_threshold_m = df['precipitation'].quantile(0.65)\n",
    "    suggested_threshold_mm = suggested_threshold_m * 1000\n",
    "    print(f\"   Suggested threshold for 35% rain events:\")\n",
    "    print(f\"   - In meters: {suggested_threshold_m:.6f} m\")\n",
    "    print(f\"   - In mm: {suggested_threshold_mm:.4f} mm\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Good class balance: {samples_above_threshold/len(df)*100:.1f}% rain events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208b900",
   "metadata": {},
   "source": [
    "## üîß Optional: Re-preprocess with Adjusted Threshold\n",
    "\n",
    "**Run this ONLY if the class distribution check above shows imbalanced data (< 10% rain events)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run preprocessing with adjusted threshold for better class balance\n",
    "# This creates a more balanced dataset by adjusting the rain threshold\n",
    "\n",
    "# Calculate appropriate threshold (aiming for ~30-40% rain events)\n",
    "df_temp = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "\n",
    "# ERA5 precipitation is in METERS\n",
    "suggested_threshold_m = df_temp['precipitation'].quantile(0.65)  # 35% will be \"rain\"\n",
    "suggested_threshold_mm = suggested_threshold_m * 1000  # Convert to mm for display\n",
    "\n",
    "print(f\"üéØ Suggested threshold:\")\n",
    "print(f\"   {suggested_threshold_m:.6f} m = {suggested_threshold_mm:.4f} mm\")\n",
    "print(f\"   This should give ~35% rain events\\n\")\n",
    "\n",
    "# Re-run preprocessing with threshold in METERS\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold {suggested_threshold_m:.6f}\n",
    "\n",
    "print(f\"\\n‚úÖ Data re-processed with adjusted threshold!\")\n",
    "print(f\"üí° Used: {suggested_threshold_m:.6f} m ({suggested_threshold_mm:.4f} mm)\")\n",
    "print(\"üìä Now check class distribution again...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6979",
   "metadata": {},
   "source": [
    "## 4. Train Timer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy pre-trained checkpoint from Google Drive\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'checkpoints/timer_xl'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint.pth'\n",
    "\n",
    "\n",
    "!mkdir -p checkpoints/timer_xl/\n",
    "\n",
    "!cp '/content/drive/MyDrive/timer_xl_peru/checkpoints/checkpoint.pth' \\\n",
    "    checkpoints/timer_xl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Transfer Learning Timer-XL (8 layers, 1024 dim) - CONFIGURACI√ìN CORREGIDA\n",
    "# ‚ö†Ô∏è OPTIMIZADO PARA 10-20 A√ëOS DE DATOS (2014-2024)\n",
    "# GPU Memory: ~5-6 GB | Training time: ~25-30 min por √©poca\n",
    "\n",
    "# CAMBIOS CR√çTICOS vs versi√≥n anterior (que usaba 10GB):\n",
    "# 1. seq_len REDUCIDO: 1440h (60 d√≠as) vs 2880h (120 d√≠as) ‚≠ê -40% VRAM\n",
    "# 2. batch_size AUMENTADO: 16 vs 12 ‚≠ê M√°s eficiente\n",
    "# 3. Learning rate sin cambios (5e-5)\n",
    "# 4. Dropout 0.2 para regularizaci√≥n\n",
    "\n",
    "# JUSTIFICACI√ìN seq_len=1440 (60 d√≠as):\n",
    "# - Captura 2 meses completos de datos\n",
    "# - Suficiente para transiciones ENSO (El Ni√±o ‚Üí Neutral ‚Üí La Ni√±a)\n",
    "# - 120 d√≠as era \"overkill\" y causaba convergencia lenta\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_timerxl_11years \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.2 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 16 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --train_epochs 30 \\\n",
    "  --patience 8 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 30 \\\n",
    "  --adaptation \\\n",
    "  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Transfer_Learning_11Years_2014_2024'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_timerxl_11years/\")\n",
    "print(\"‚è±Ô∏è Tiempo esperado: 12-15 horas (30 √©pocas √ó 25-30 min)\")\n",
    "print(\"üéØ Meta: F1-Score > 0.82\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f743a57",
   "metadata": {},
   "source": [
    "## üîß Transfer Learning MEJORADO - Versi√≥n 2 (Aprendizaje M√°s Lento)\n",
    "\n",
    "**‚ö†Ô∏è CAMBIOS CR√çTICOS vs Versi√≥n 1 (que no converg√≠a)**:\n",
    "1. Learning rate REDUCIDO: 1e-5 (antes 5e-5) ‚≠ê Convergencia m√°s estable\n",
    "2. Batch size REDUCIDO: 12 (antes 16) ‚≠ê Actualizaciones m√°s frecuentes\n",
    "3. Warmup epochs: 3 ‚≠ê Adaptaci√≥n gradual de pretrained weights\n",
    "4. Dropout REDUCIDO: 0.15 (antes 0.2) ‚≠ê Permite usar conocimiento pretrained\n",
    "\n",
    "**Justificaci√≥n**:\n",
    "- Versi√≥n 1 colaps√≥ en clase mayoritaria (Recall=1.0, predice \"rain\" siempre)\n",
    "- Learning rate 5e-5 era muy agresivo para transfer learning\n",
    "- 1e-5 permite ajuste fino gradual de weights preentrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning Timer-XL - VERSI√ìN 2 MEJORADA\n",
    "# Learning rate M√ÅS BAJO para convergencia estable\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_timerxl_11years_v2 \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.15 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 12 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --train_epochs 40 \\\n",
    "  --patience 10 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 40 \\\n",
    "  --adaptation \\\n",
    "  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Transfer_Learning_V2_Stable_11Years'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_timerxl_11years_v2/\")\n",
    "print(\"‚è±Ô∏è Tiempo esperado: 18-20 horas (40 √©pocas √ó 27-30 min)\")\n",
    "print(\"üéØ Meta: F1-Score > 0.82 con convergencia estable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316bfe3",
   "metadata": {},
   "source": [
    "## üî¨ Option A: Train from Scratch (NO transfer learning)\n",
    "\n",
    "**Use this if transfer learning keeps producing NaN loss**  \n",
    "This will verify if the model architecture itself works with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch WITHOUT pretrained weights\n",
    "# This is faster to converge and more stable for classification\n",
    "# ‚ö†Ô∏è USES CLEANED DATA (peru_rainfall_cleaned.csv)\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_scratch \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.1 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 16 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --train_epochs 50 \\\n",
    "  --patience 10 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 50 \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_From_Scratch_Cleaned'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_scratch/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdd64",
   "metadata": {},
   "source": [
    "## üî¨ Option B: Smaller Model (More Stable)\n",
    "\n",
    "**Faster training and more stable** - Use this if Option A also has issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be14cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Small Model EFICIENTE (5 layers, 640 dim) - CONFIGURACI√ìN CORREGIDA\n",
    "# ‚ö†Ô∏è OPTIMIZADO PARA 10-20 A√ëOS DE DATOS (2014-2024)\n",
    "# GPU Memory: ~3-4 GB | Training time: ~8-10 min por √©poca\n",
    "\n",
    "# CAMBIOS CR√çTICOS vs versi√≥n anterior (que usaba 6GB):\n",
    "# 1. e_layers REDUCIDO: 5 layers vs 6 layers ‚≠ê -33% VRAM\n",
    "# 2. d_model REDUCIDO: 640 vs 768 ‚≠ê Punto medio entre 512 y 768\n",
    "# 3. d_ff AJUSTADO: 1280 (proporcional a d_model)\n",
    "# 4. batch_size AUMENTADO: 32 vs 24 ‚≠ê M√°s eficiente\n",
    "# 5. seq_len SIN CAMBIOS: 1440h (60 d√≠as) - Ya estaba bien\n",
    "\n",
    "# JUSTIFICACI√ìN (5 layers, 640 dim):\n",
    "# - Versi√≥n anterior (6L, 768D) = 6GB VRAM (casi como Transfer Learning!)\n",
    "# - Esta versi√≥n (5L, 640D) = 3-4GB VRAM (realmente \"small\")\n",
    "# - Mantiene capacidad suficiente para 11 a√±os de datos\n",
    "# - Permite entrenamientos r√°pidos (3-4 horas total)\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_small_efficient_11years \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 5 \\\n",
    "  --d_model 640 \\\n",
    "  --d_ff 1280 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.15 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 8e-5 \\\n",
    "  --train_epochs 25 \\\n",
    "  --patience 8 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 25 \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Small_Efficient_11Years_2014_2024'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_small_improved_11years/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b3652",
   "metadata": {},
   "source": [
    "## üîÑ \"CONTINUAR\" Small Model desde Epoch 6 (Workaround)\n",
    "\n",
    "**‚ö†Ô∏è LIMITACI√ìN: Timer-XL NO soporta continuaci√≥n nativa**\n",
    "\n",
    "Timer-XL solo guarda `state_dict` (weights), NO guarda optimizer ni epoch counter.\n",
    "\n",
    "**SOLUCI√ìN: Usa tu checkpoint como pretrained weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Continuar Small Model usando checkpoint como pretrained weights\n",
    "# ‚ö†Ô∏è DET√âN el entrenamiento actual si est√° corriendo (bot√≥n STOP)\n",
    "\n",
    "# PASO 1: Buscar tu checkpoint guardado\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Buscar el checkpoint de √©poca 6\n",
    "checkpoint_pattern = \"checkpoints/classification_peru_rainfall_small_efficient_11years_*/checkpoint.pth\"\n",
    "checkpoints_found = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if checkpoints_found:\n",
    "    CHECKPOINT_PATH = checkpoints_found[0]\n",
    "    print(f\"‚úÖ Checkpoint encontrado: {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontr√≥ checkpoint. Verifica la ruta.\")\n",
    "    print(\"Archivos en checkpoints/:\")\n",
    "    !ls -lh checkpoints/\n",
    "\n",
    "# PASO 2: Entrenar 19 √©pocas adicionales usando el checkpoint\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_small_continue \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 5 \\\n",
    "  --d_model 640 \\\n",
    "  --d_ff 1280 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.15 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 8e-5 \\\n",
    "  --train_epochs 19 \\\n",
    "  --patience 8 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 19 \\\n",
    "  --adaptation \\\n",
    "  --pretrain_model_path $CHECKPOINT_PATH \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Small_Continue_From_Epoch6'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_small_continue/\")\n",
    "print(\"‚è±Ô∏è Tiempo: 2.5-3 horas (19 √©pocas √ó 8-10 min)\")\n",
    "print(\"üéØ √âpocas efectivas: 6 (previo) + 19 (nuevo) = 25 total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed72df",
   "metadata": {},
   "source": [
    "## üîß FASE 1: Rescate del Modelo (Data Augmentation + Focal Loss Ajustado)\n",
    "\n",
    "**‚ö†Ô∏è PROBLEMA DETECTADO**: Fine-tuning caus√≥ overfitting severo\n",
    "- Recall No Rain: 76% ‚Üí 32% ‚ùå (colaps√≥ -58%)\n",
    "- Modelo predice \"Rain\" por defecto (Recall Rain 91%)\n",
    "\n",
    "**SOLUCI√ìN**:\n",
    "1. **Focal Loss M√ÅS AGRESIVO**: alpha=0.70 (antes 0.66), gamma=3.0 (antes 2.5)\n",
    "2. **Regularizaci√≥n aumentada**: dropout=0.25 (antes 0.15)\n",
    "3. **Class weights adicionales**: [0.4, 0.6] para balancear hard\n",
    "4. **Learning rate intermedio**: 5e-5 (antes 2e-5 muy lento, 8e-5 muy r√°pido)\n",
    "\n",
    "**OBJETIVO**: Recuperar F1 > 0.80, Recall No Rain > 60%\n",
    "\n",
    "**RESULTADOS ANTERIORES**:\n",
    "```\n",
    "√âpoca 8 (checkpoint inicial):\n",
    "  F1=0.7827, Recall No Rain=76%, Recall Rain=73% ‚úÖ\n",
    "\n",
    "√âpoca 2 (fine-tuning fallido):\n",
    "  F1=0.7929, Recall No Rain=32%, Recall Rain=91% ‚ùå OVERFITTING\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0306b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• FASE 1: Rescate con Data Augmentation + Focal Loss Agresivo\n",
    "# ENTRENAR DESDE CERO con configuraci√≥n balanceada\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ FASE 1: RESCATE DEL MODELO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìã CAMBIOS APLICADOS:\")\n",
    "print(\"   ‚Ä¢ Focal Loss: alpha=0.70 (favorece 'No Rain'), gamma=3.0\")\n",
    "print(\"   ‚Ä¢ Dropout: 0.25 (regularizaci√≥n fuerte)\")\n",
    "print(\"   ‚Ä¢ Learning rate: 5e-5 (intermedio)\")\n",
    "print(\"   ‚Ä¢ Entrenar desde CERO (no usar checkpoint fallido)\")\n",
    "print(\"\\nüéØ OBJETIVO:\")\n",
    "print(\"   ‚Ä¢ F1-Score > 0.80\")\n",
    "print(\"   ‚Ä¢ Recall No Rain > 60% (actualmente 32%)\")\n",
    "print(\"   ‚Ä¢ Recall Rain ~80-85%\")\n",
    "print(\"   ‚Ä¢ Balance entre clases\\n\")\n",
    "\n",
    "!python run.py \\\n",
    "    --task_name classification \\\n",
    "    --is_training 1 \\\n",
    "    --model_id peru_rainfall_focal_rescue_v1 \\\n",
    "    --model timer_xl_classifier \\\n",
    "    --data PeruRainfall \\\n",
    "    --root_path datasets/processed/ \\\n",
    "    --data_path peru_rainfall_cleaned.csv \\\n",
    "    --checkpoints checkpoints/ \\\n",
    "    --seq_len 1440 \\\n",
    "    --input_token_len 96 \\\n",
    "    --output_token_len 96 \\\n",
    "    --test_seq_len 1440 \\\n",
    "    --test_pred_len 2 \\\n",
    "    --e_layers 5 \\\n",
    "    --d_model 640 \\\n",
    "    --d_ff 1280 \\\n",
    "    --n_heads 8 \\\n",
    "    --dropout 0.25 \\\n",
    "    --activation relu \\\n",
    "    --batch_size 32 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --train_epochs 20 \\\n",
    "    --patience 6 \\\n",
    "    --n_classes 2 \\\n",
    "    --gpu 0 \\\n",
    "    --cosine \\\n",
    "    --tmax 20 \\\n",
    "    --use_focal_loss \\\n",
    "    --focal_alpha 0.70 \\\n",
    "    --focal_gamma 3.0 \\\n",
    "    --loss CE \\\n",
    "    --itr 1 \\\n",
    "    --des 'Peru_Focal_Rescue_Alpha070_Gamma3'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ FASE 1 COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüîç VERIFICAR:\")\n",
    "print(\"   ‚Ä¢ Confusion Matrix: ¬øRecall No Rain > 60%?\")\n",
    "print(\"   ‚Ä¢ F1-Score: ¬ø> 0.80?\")\n",
    "print(\"   ‚Ä¢ Balance: ¬ø|Recall No Rain - Recall Rain| < 20%?\")\n",
    "print(\"\\nüìä Si los resultados son buenos, pasar a FASE 2 (validaci√≥n ENSO-aware)\")\n",
    "print(\"üí° Si a√∫n hay sesgo, aumentar alpha a 0.75 o gamma a 3.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992ef2c",
   "metadata": {},
   "source": [
    "## üåä FASE 2: Validaci√≥n ENSO-aware (Core de tu Tesis)\n",
    "\n",
    "**Objetivo**: Evaluar rendimiento del modelo por fases ENSO.\n",
    "\n",
    "**Hip√≥tesis a validar**:\n",
    "1. **H1**: F1 > 0.75 en TODAS las fases (El Ni√±o, La Ni√±a, Neutral)\n",
    "2. **H2**: |F1_ElNi√±o - F1_LaNi√±a| < 0.15 (consistencia)\n",
    "3. **H3**: F1_ElNi√±o ‚â• F1_Neutral AND F1_LaNi√±a ‚â• F1_Neutral\n",
    "\n",
    "**Requisito previo**: Haber completado FASE 1 con F1 > 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeedaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåä FASE 2: Ejecutar Validaci√≥n ENSO-aware\n",
    "# ‚ö†Ô∏è PRIMERO: Aseg√∫rate de tener un checkpoint con F1 > 0.80\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üåä FASE 2: VALIDACI√ìN ENSO-AWARE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Buscar mejor checkpoint de FASE 1\n",
    "checkpoint_pattern = \"checkpoints/classification_peru_rainfall_focal_rescue_v1_*/checkpoint.pth\"\n",
    "checkpoints = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if not checkpoints:\n",
    "    print(\"\\n‚ùå ERROR: No se encontr√≥ checkpoint de FASE 1\")\n",
    "    print(\"   Ejecuta primero la celda de FASE 1 (Rescate del Modelo)\")\n",
    "    print(\"   Debe generar un checkpoint con F1 > 0.80\\n\")\n",
    "else:\n",
    "    checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "    CHECKPOINT_PATH = checkpoints[0]\n",
    "    CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_PATH)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Checkpoint encontrado: {CHECKPOINT_DIR}\")\n",
    "    \n",
    "    # Nota: validate_enso_phases.py requiere integraci√≥n con tu pipeline\n",
    "    # Por ahora, ejecuta el test normal y guarda predicciones\n",
    "    \n",
    "    print(\"\\n\udcca PASO 1: Generar predicciones del modelo...\")\n",
    "    print(\"   (Debes ejecutar el test y guardar predicciones con timestamps)\")\n",
    "    \n",
    "    # Ejecutar test y guardar predicciones\n",
    "    !python test_checkpoint_standalone.py \\\n",
    "        --checkpoint_path $CHECKPOINT_PATH \\\n",
    "        --save_predictions \\\n",
    "        --output_dir results/enso_validation\n",
    "    \n",
    "    print(\"\\nüìä PASO 2: Ejecutar an√°lisis ENSO-aware...\")\n",
    "    \n",
    "    # ‚ö†Ô∏è REQUIERE ADAPTACI√ìN: validate_enso_phases.py necesita acceso a predicciones\n",
    "    # Por ahora, placeholder - debes integrar con tu pipeline\n",
    "    \n",
    "    print(\"\\nüí° SIGUIENTE PASO:\")\n",
    "    print(\"   1. Revisa el archivo de predicciones generado\")\n",
    "    print(\"   2. A√±ade columna 'enso_phase' al CSV de predicciones\")\n",
    "    print(\"   3. Ejecuta: !python validate_enso_phases.py \\\\\")\n",
    "    print(\"              --data_path results/enso_validation/predictions_with_phases.csv \\\\\")\n",
    "    print(\"              --output_dir results/enso_validation\")\n",
    "    \n",
    "    print(\"\\nüìä M√âTRICAS ESPERADAS:\")\n",
    "    print(\"   ‚úÖ F1 El Ni√±o > 0.75\")\n",
    "    print(\"   ‚úÖ F1 La Ni√±a > 0.75\")\n",
    "    print(\"   ‚úÖ F1 Neutral > 0.75\")\n",
    "    print(\"   ‚úÖ |F1_ElNi√±o - F1_LaNi√±a| < 0.15\")\n",
    "    \n",
    "    print(\"\\nüìÅ Resultados se guardar√°n en: results/enso_validation/\")\n",
    "    print(\"   - enso_f1_comparison.png\")\n",
    "    print(\"   - enso_confusion_matrices.png\")\n",
    "    print(\"   - enso_validation_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263aa5a4",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è FASE 3: An√°lisis Regional (Costa Norte vs Centro vs Sur)\n",
    "\n",
    "**Objetivo**: Validar gradiente de influencia ENSO.\n",
    "\n",
    "**Hip√≥tesis a validar**:\n",
    "1. **H4**: F1_Norte > F1_Centro > F1_Sur (gradiente ENSO)\n",
    "2. **H5**: Rain_prevalence_Norte > Rain_prevalence_Sur\n",
    "\n",
    "**Requisito previo**: Haber completado FASE 1 y FASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üó∫Ô∏è FASE 3: Ejecutar An√°lisis Regional\n",
    "# ‚ö†Ô∏è REQUIERE: Datos con coordenadas geogr√°ficas (latitud, longitud)\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üó∫Ô∏è FASE 3: AN√ÅLISIS REGIONAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar que existan predicciones con coordenadas\n",
    "predictions_file = \"results/enso_validation/predictions_with_coords.csv\"\n",
    "\n",
    "if not os.path.exists(predictions_file):\n",
    "    print(\"\\n‚ö†Ô∏è NOTA: Se requiere CSV con predicciones + coordenadas\")\n",
    "    print(\"   Columnas necesarias:\")\n",
    "    print(\"   - timestamp\")\n",
    "    print(\"   - latitude (para asignar regi√≥n)\")\n",
    "    print(\"   - rain_24h (label verdadero)\")\n",
    "    print(\"   - pred_label (predicci√≥n del modelo)\")\n",
    "    print(\"   - pred_proba_rain (probabilidad clase Rain)\")\n",
    "    \n",
    "    print(\"\\nüí° CREAR CSV:\")\n",
    "    print(\"   1. Cargar datos originales (peru_rainfall_cleaned.csv)\")\n",
    "    print(\"   2. A√±adir columnas de predicci√≥n del modelo\")\n",
    "    print(\"   3. Guardar como predictions_with_coords.csv\")\n",
    "    \n",
    "    print(\"\\nüìä REGIONES (basado en latitud):\")\n",
    "    print(\"   - Costa Norte (-8¬∞ a -4¬∞): Piura, Tumbes, Lambayeque\")\n",
    "    print(\"   - Costa Centro (-14¬∞ a -8¬∞): Lima, Callao, Ica\")\n",
    "    print(\"   - Costa Sur (-18¬∞ a -14¬∞): Arequipa, Moquegua, Tacna\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Archivo de predicciones encontrado: {predictions_file}\")\n",
    "    \n",
    "    print(\"\\nüìä Ejecutando an√°lisis regional...\")\n",
    "    \n",
    "    !python validate_regional.py \\\n",
    "        --data_path $predictions_file \\\n",
    "        --output_dir results/regional_analysis\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ AN√ÅLISIS REGIONAL COMPLETADO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä VERIFICAR HIP√ìTESIS:\")\n",
    "    print(\"   ‚úÖ H4: ¬øF1_Norte > F1_Centro > F1_Sur?\")\n",
    "    print(\"   ‚úÖ H5: ¬øRain_prevalence_Norte > Rain_prevalence_Sur?\")\n",
    "    \n",
    "    print(\"\\nüìÅ Resultados guardados en: results/regional_analysis/\")\n",
    "    print(\"   - regional_comparison.png\")\n",
    "    print(\"   - regional_confusion_matrices.png\")\n",
    "    print(\"   - regional_analysis_report.txt\")\n",
    "    \n",
    "    print(\"\\nüí° INTERPRETACI√ìN:\")\n",
    "    print(\"   Si H4 se cumple ‚Üí Timer-XL captura gradiente ENSO ‚úÖ\")\n",
    "    print(\"   Si H4 NO se cumple ‚Üí Requiere features ENSO expl√≠citos ‚ö†Ô∏è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae69d19",
   "metadata": {},
   "source": [
    "## 5. Save Checkpoint to Drive\n",
    "\n",
    "Prevent losing your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63515cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy training results to Google Drive (prevent losing trained model!)\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Find the checkpoint directory\n",
    "checkpoint_base = 'checkpoints'\n",
    "results_pattern = f'{checkpoint_base}/*/peru_rainfall_timerxl*/'\n",
    "\n",
    "matching_dirs = glob.glob(results_pattern)\n",
    "\n",
    "if matching_dirs:\n",
    "    results_path = matching_dirs[0]\n",
    "    \n",
    "    # Copy entire results folder to Drive\n",
    "    drive_results = '/content/drive/MyDrive/timer_xl_peru/results/'\n",
    "    os.makedirs(drive_results, exist_ok=True)\n",
    "    \n",
    "    print(\"üíæ Copying results to Google Drive...\")\n",
    "    print(f\"   From: {results_path}\")\n",
    "    print(f\"   To: {drive_results}\")\n",
    "    \n",
    "    # Use shutil for better error handling\n",
    "    try:\n",
    "        shutil.copytree(results_path, os.path.join(drive_results, os.path.basename(results_path.rstrip('/'))), dirs_exist_ok=True)\n",
    "        print(\"‚úÖ Checkpoint and results saved to Google Drive!\")\n",
    "        print(f\"üìÅ Location: {drive_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error copying to Drive: {e}\")\n",
    "        print(\"   You can manually copy from:\", results_path)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results found. Training may have failed or is still in progress.\")\n",
    "    print(\"   Expected pattern:\", results_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699a1c1",
   "metadata": {},
   "source": [
    "## üéØ TEST ANY CHECKPOINT (Transfer Learning, Small, or From Scratch)\n",
    "\n",
    "**Use este script standalone para testear cualquier checkpoint .pth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a724c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Test ANY checkpoint with the standalone script\n",
    "# This automatically finds and tests the latest checkpoint\n",
    "\n",
    "!python test_checkpoint_standalone.py --find_latest\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° TIP: Para testear un checkpoint espec√≠fico, usa:\")\n",
    "print(\"   !python test_checkpoint_standalone.py --checkpoint_path 'ruta/al/checkpoint.pth'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba25f5",
   "metadata": {},
   "source": [
    "## üíæ GUARDAR CHECKPOINTS ANTES DE DESCONECTAR\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE: Ejecuta esta celda ANTES de desconectar Colab para no perder tu progreso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Backup autom√°tico de checkpoints a Google Drive\n",
    "# Ejecuta esta celda ANTES de desconectar Colab para guardar todo tu progreso\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üíæ GUARDANDO CHECKPOINTS A GOOGLE DRIVE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Directorio de destino en Drive\n",
    "drive_backup = '/content/drive/MyDrive/timer_xl_peru/checkpoints_backup/'\n",
    "os.makedirs(drive_backup, exist_ok=True)\n",
    "\n",
    "# Timestamp para identificar este backup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Buscar TODOS los checkpoints generados\n",
    "checkpoint_patterns = [\n",
    "    'checkpoints/classification_peru_rainfall_timerxl_11years_*/',\n",
    "    'checkpoints/classification_peru_rainfall_small_improved_11years_*/',\n",
    "    'checkpoints/classification_peru_rainfall_timerxl_*/',\n",
    "    'checkpoints/classification_peru_rainfall_small_*/'\n",
    "]\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "for pattern in checkpoint_patterns:\n",
    "    matching_dirs = glob.glob(pattern)\n",
    "    \n",
    "    for checkpoint_dir in matching_dirs:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            # Nombre descriptivo para el backup\n",
    "            model_name = os.path.basename(checkpoint_dir.rstrip('/'))\n",
    "            backup_name = f\"{model_name}_{timestamp}.pth\"\n",
    "            backup_path = os.path.join(drive_backup, backup_name)\n",
    "            \n",
    "            # Copiar checkpoint\n",
    "            print(f\"üì¶ Guardando: {model_name}\")\n",
    "            print(f\"   Origen: {checkpoint_path}\")\n",
    "            print(f\"   Destino: {backup_path}\")\n",
    "            \n",
    "            try:\n",
    "                shutil.copy2(checkpoint_path, backup_path)\n",
    "                \n",
    "                # Obtener tama√±o del archivo\n",
    "                size_mb = os.path.getsize(backup_path) / (1024**2)\n",
    "                print(f\"   ‚úÖ Guardado exitoso ({size_mb:.1f} MB)\\n\")\n",
    "                \n",
    "                saved_models.append({\n",
    "                    'name': model_name,\n",
    "                    'path': backup_path,\n",
    "                    'size_mb': size_mb\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"=\"*80)\n",
    "print(\"üìä RESUMEN DEL BACKUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if saved_models:\n",
    "    print(f\"\\n‚úÖ {len(saved_models)} checkpoint(s) guardado(s):\\n\")\n",
    "    \n",
    "    total_size = 0\n",
    "    for model in saved_models:\n",
    "        print(f\"   ‚Ä¢ {model['name']}\")\n",
    "        print(f\"     Tama√±o: {model['size_mb']:.1f} MB\")\n",
    "        print(f\"     Ubicaci√≥n: {model['path']}\\n\")\n",
    "        total_size += model['size_mb']\n",
    "    \n",
    "    print(f\"üíæ Tama√±o total: {total_size:.1f} MB\")\n",
    "    print(f\"üìÅ Directorio: {drive_backup}\")\n",
    "    \n",
    "    # Guardar tambi√©n metadata\n",
    "    metadata_path = os.path.join(drive_backup, f'backup_metadata_{timestamp}.txt')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(f\"Backup realizado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total checkpoints: {len(saved_models)}\\n\")\n",
    "        f.write(f\"Tama√±o total: {total_size:.1f} MB\\n\\n\")\n",
    "        f.write(\"Checkpoints guardados:\\n\")\n",
    "        for model in saved_models:\n",
    "            f.write(f\"  - {model['name']} ({model['size_mb']:.1f} MB)\\n\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Metadata guardada: {metadata_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se encontraron checkpoints para guardar.\")\n",
    "    print(\"   Verifica que el entrenamiento haya generado checkpoints.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ BACKUP COMPLETADO - Ya puedes desconectar Colab de forma segura\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
