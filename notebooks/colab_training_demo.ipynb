{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb4827",
   "metadata": {},
   "source": [
    "# 🌧️ Timer-XL Peru Rainfall Prediction - Google Colab\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training Timer-XL on Peru rainfall data.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup environment\n",
    "2. Upload ERA5 data\n",
    "3. Preprocess data\n",
    "4. Train Timer-XL with transfer learning\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb58468",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660775a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/ChristianPE1/test-openltm-code.git\n",
    "%cd test-openltm-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b831b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92910c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (to download checkpoint.pth and save training results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✅ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524612e",
   "metadata": {},
   "source": [
    "## 2. Verificar Datos ERA5\n",
    "\n",
    "**Los archivos .nc ya están en el repositorio** (datasets/raw_era5/)  \n",
    "Solo necesitas verificar que se clonaron correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ERA5 files are in the repository\n",
    "!ls -lh datasets/raw_era5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9543ac7",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing script\n",
    "# ⚠️ IMPORTANT: ERA5 precipitation is in METERS, not millimeters!\n",
    "# Use threshold in METERS: 0.1 mm = 0.0001 m\n",
    "\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold 0.0001\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete!\")\n",
    "print(\"📊 Output files saved to: datasets/processed/\")\n",
    "print(\"💡 Threshold: 0.0001 m = 0.1 mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for quick inspection\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Load statistics\n",
    "with open('datasets/processed/preprocessing_stats.json') as f:\n",
    "    stats = json.load(f)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a55ac",
   "metadata": {},
   "source": [
    "## 🚨 CRITICAL: Verify Class Balance\n",
    "\n",
    "**Before training, we MUST check that both classes exist!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Check class distribution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "\n",
    "print(\"📊 Class Distribution Analysis:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   rain_24h column:\")\n",
    "print(df['rain_24h'].value_counts())\n",
    "print(f\"\\n   Percentage:\")\n",
    "print(df['rain_24h'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Check precipitation values\n",
    "print(f\"\\n🌧️ Precipitation Statistics (in METERS from ERA5):\")\n",
    "print(f\"   Min: {df['precipitation'].min():.6f} m = {df['precipitation'].min()*1000:.3f} mm\")\n",
    "print(f\"   Max: {df['precipitation'].max():.6f} m = {df['precipitation'].max()*1000:.3f} mm\")\n",
    "print(f\"   Mean: {df['precipitation'].mean():.6f} m = {df['precipitation'].mean()*1000:.3f} mm\")\n",
    "print(f\"   Median: {df['precipitation'].median():.6f} m = {df['precipitation'].median()*1000:.3f} mm\")\n",
    "print(f\"   95th percentile: {df['precipitation'].quantile(0.95):.6f} m = {df['precipitation'].quantile(0.95)*1000:.3f} mm\")\n",
    "\n",
    "# ⚠️ IMPORTANT: ERA5 precipitation is in METERS, not millimeters!\n",
    "# Threshold must be in meters too\n",
    "threshold_mm = 0.1  # Target in mm\n",
    "threshold_m = threshold_mm / 1000.0  # Convert to meters\n",
    "\n",
    "samples_above_threshold = (df['precipitation'] >= threshold_m).sum()\n",
    "print(f\"\\n⚠️  Samples with precipitation >= {threshold_mm} mm ({threshold_m:.6f} m): {samples_above_threshold} ({samples_above_threshold/len(df)*100:.2f}%)\")\n",
    "\n",
    "if samples_above_threshold < len(df) * 0.1:\n",
    "    print(f\"\\n⚠️ Class imbalance detected!\")\n",
    "    print(f\"   Only {samples_above_threshold} rain events ({samples_above_threshold/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\n💡 SOLUTION:\")\n",
    "    # Calculate threshold for 30-35% rain events\n",
    "    suggested_threshold_m = df['precipitation'].quantile(0.65)\n",
    "    suggested_threshold_mm = suggested_threshold_m * 1000\n",
    "    print(f\"   Suggested threshold for 35% rain events:\")\n",
    "    print(f\"   - In meters: {suggested_threshold_m:.6f} m\")\n",
    "    print(f\"   - In mm: {suggested_threshold_mm:.4f} mm\")\n",
    "else:\n",
    "    print(f\"\\n✅ Good class balance: {samples_above_threshold/len(df)*100:.1f}% rain events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208b900",
   "metadata": {},
   "source": [
    "## 🔧 Optional: Re-preprocess with Adjusted Threshold\n",
    "\n",
    "**Run this ONLY if the class distribution check above shows imbalanced data (< 10% rain events)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run preprocessing with adjusted threshold for better class balance\n",
    "# This creates a more balanced dataset by adjusting the rain threshold\n",
    "\n",
    "# Calculate appropriate threshold (aiming for ~30-40% rain events)\n",
    "df_temp = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "\n",
    "# ERA5 precipitation is in METERS\n",
    "suggested_threshold_m = df_temp['precipitation'].quantile(0.65)  # 35% will be \"rain\"\n",
    "suggested_threshold_mm = suggested_threshold_m * 1000  # Convert to mm for display\n",
    "\n",
    "print(f\"🎯 Suggested threshold:\")\n",
    "print(f\"   {suggested_threshold_m:.6f} m = {suggested_threshold_mm:.4f} mm\")\n",
    "print(f\"   This should give ~35% rain events\\n\")\n",
    "\n",
    "# Re-run preprocessing with threshold in METERS\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold {suggested_threshold_m:.6f}\n",
    "\n",
    "print(f\"\\n✅ Data re-processed with adjusted threshold!\")\n",
    "print(f\"💡 Used: {suggested_threshold_m:.6f} m ({suggested_threshold_mm:.4f} mm)\")\n",
    "print(\"📊 Now check class distribution again...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6979",
   "metadata": {},
   "source": [
    "## 4. Train Timer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy pre-trained checkpoint from Google Drive\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'checkpoints/timer_xl'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint.pth'\n",
    "\n",
    "\n",
    "!mkdir -p checkpoints/timer_xl/\n",
    "\n",
    "!cp '/content/drive/MyDrive/timer_xl_peru/checkpoints/checkpoint.pth' \\\n",
    "    checkpoints/timer_xl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Timer-XL with transfer learning\n",
    "# ⚠️ NOW USING CLEANED DATA (peru_rainfall_cleaned.csv)\n",
    "# This will take 4-6 hours on T4 GPU\n",
    "\n",
    "# IMPORTANT: Using low learning rate (1e-5) for numerical stability\n",
    "# With cleaned data, transfer learning should work now!\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_timerxl \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.1 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 16 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --train_epochs 50 \\\n",
    "  --patience 10 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 50 \\\n",
    "  --adaptation \\\n",
    "  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Transfer_Learning_Cleaned'\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "print(\"📊 Results saved to: checkpoints/peru_rainfall_timerxl/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316bfe3",
   "metadata": {},
   "source": [
    "## 🔬 Option A: Train from Scratch (NO transfer learning)\n",
    "\n",
    "**Use this if transfer learning keeps producing NaN loss**  \n",
    "This will verify if the model architecture itself works with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from scratch WITHOUT pretrained weights\n",
    "# This is faster to converge and more stable for classification\n",
    "# ⚠️ USES CLEANED DATA (peru_rainfall_cleaned.csv)\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_scratch \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.1 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 16 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --train_epochs 50 \\\n",
    "  --patience 10 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 50 \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_From_Scratch_Cleaned'\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "print(\"📊 Results saved to: checkpoints/peru_rainfall_scratch/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdd64",
   "metadata": {},
   "source": [
    "## 🔬 Option B: Smaller Model (More Stable)\n",
    "\n",
    "**Faster training and more stable** - Use this if Option A also has issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be14cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller Timer-XL model (4 layers instead of 8)\n",
    "# Much faster and more stable for binary classification\n",
    "# ⚠️ USES CLEANED DATA (peru_rainfall_cleaned.csv)\n",
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_small \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 720 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 720 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 4 \\\n",
    "  --d_model 512 \\\n",
    "  --d_ff 1024 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.1 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --train_epochs 30 \\\n",
    "  --patience 10 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Small_Model_Cleaned'\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "print(\"📊 Results saved to: checkpoints/peru_rainfall_small/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c901e27",
   "metadata": {},
   "source": [
    "## 📊 Re-test Small Model (Ver Precision/Recall)\n",
    "\n",
    "**El Small Model ya entrenó, solo falta ver métricas completas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint from Small Model training and run complete test\n",
    "# This will show Precision, Recall, F1-Score, and Confusion Matrix\n",
    "\n",
    "import glob\n",
    "\n",
    "# Find the checkpoint directory for Small Model\n",
    "checkpoint_pattern = 'checkpoints/classification_peru_rainfall_small_*/'\n",
    "checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    checkpoint_dir = checkpoint_dirs[0]\n",
    "    print(f\"📁 Found checkpoint: {checkpoint_dir}\")\n",
    "    \n",
    "    # Run test with the trained model\n",
    "    !python run.py \\\n",
    "      --task_name classification \\\n",
    "      --is_training 0 \\\n",
    "      --model_id peru_rainfall_small \\\n",
    "      --model timer_xl_classifier \\\n",
    "      --data PeruRainfall \\\n",
    "      --root_path datasets/processed/ \\\n",
    "      --data_path peru_rainfall_cleaned.csv \\\n",
    "      --checkpoints checkpoints/ \\\n",
    "      --seq_len 720 \\\n",
    "      --input_token_len 96 \\\n",
    "      --output_token_len 96 \\\n",
    "      --test_seq_len 720 \\\n",
    "      --test_pred_len 2 \\\n",
    "      --e_layers 4 \\\n",
    "      --d_model 512 \\\n",
    "      --d_ff 1024 \\\n",
    "      --n_heads 8 \\\n",
    "      --n_classes 2 \\\n",
    "      --gpu 0 \\\n",
    "      --itr 1\n",
    "else:\n",
    "    print(\"⚠️ No checkpoint found. Train the Small Model first.\")\n",
    "    print(f\"   Expected pattern: {checkpoint_pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b3652",
   "metadata": {},
   "source": [
    "## 🐛 DEBUG: Check Data Quality\n",
    "\n",
    "**CRITICAL - Run this BEFORE Option A/B to identify the root cause**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeedaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive data quality check\n",
    "!python debug_data_quality.py\n",
    "\n",
    "print(\"\\n🔬 ADDITIONAL CHECKS:\")\n",
    "\n",
    "# Check if data can be loaded without NaN\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "\n",
    "# Drop non-feature columns\n",
    "feature_cols = [col for col in df.columns if col not in ['rain_24h', 'timestamp', 'region', 'precipitation']]\n",
    "X = df[feature_cols].values\n",
    "\n",
    "print(f\"\\n📊 Feature Matrix:\")\n",
    "print(f\"   Shape: {X.shape}\")\n",
    "print(f\"   Contains NaN: {np.isnan(X).any()}\")\n",
    "print(f\"   Contains Inf: {np.isinf(X).any()}\")\n",
    "print(f\"   Min: {X.min():.6f}, Max: {X.max():.6f}\")\n",
    "\n",
    "# Try normalizing (like the model does)\n",
    "means = X.mean(axis=0, keepdims=True)\n",
    "stds = X.std(axis=0, keepdims=True)\n",
    "print(f\"\\n📉 Normalization Stats:\")\n",
    "print(f\"   Zero std features: {(stds == 0).sum()}\")\n",
    "print(f\"   Very small std (<1e-5): {(stds < 1e-5).sum()}\")\n",
    "\n",
    "if (stds < 1e-5).sum() > 0:\n",
    "    print(\"\\n❌ FOUND THE PROBLEM!\")\n",
    "    print(\"   Some features have near-zero standard deviation.\")\n",
    "    print(\"   This causes division by zero → NaN in normalization\")\n",
    "    zero_std_cols = [feature_cols[i] for i in np.where(stds[0] < 1e-5)[0]]\n",
    "    print(f\"   Problematic features: {zero_std_cols}\")\n",
    "else:\n",
    "    print(\"\\n✅ Normalization should be safe\")\n",
    "\n",
    "# Test conversion to torch tensor\n",
    "try:\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    print(f\"\\n✅ Torch conversion successful\")\n",
    "    print(f\"   Has NaN: {torch.isnan(X_tensor).any()}\")\n",
    "    print(f\"   Has Inf: {torch.isinf(X_tensor).any()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Torch conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263aa5a4",
   "metadata": {},
   "source": [
    "## 🧹 CLEAN DATA (if issues found)\n",
    "\n",
    "**Run this if the debug check above found NaN/Inf/zero-variance issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data to fix NaN/Inf/zero-variance issues\n",
    "!python clean_data.py \\\n",
    "    --input datasets/processed/peru_rainfall.csv \\\n",
    "    --output datasets/processed/peru_rainfall_cleaned.csv\n",
    "\n",
    "print(\"\\n✅ Data cleaned!\")\n",
    "print(\"💡 Now use --data_path peru_rainfall_cleaned.csv in training commands\")\n",
    "\n",
    "# Verify cleaning worked\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_clean = pd.read_csv('datasets/processed/peru_rainfall_cleaned.csv')\n",
    "feature_cols = [col for col in df_clean.columns if col not in ['rain_24h', 'timestamp', 'region']]\n",
    "\n",
    "print(f\"\\n📊 Cleaned data validation:\")\n",
    "print(f\"   Shape: {df_clean.shape}\")\n",
    "print(f\"   NaN values: {df_clean[feature_cols].isnull().sum().sum()}\")\n",
    "print(f\"   Inf values: {sum([np.isinf(df_clean[col]).sum() for col in feature_cols])}\")\n",
    "\n",
    "# Check std > 0\n",
    "stds = df_clean[feature_cols].std()\n",
    "zero_std = (stds < 1e-8).sum()\n",
    "print(f\"   Zero variance features: {zero_std}\")\n",
    "\n",
    "if zero_std == 0:\n",
    "    print(\"\\n✅ Data is clean and ready for training!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Still has {zero_std} problematic features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae69d19",
   "metadata": {},
   "source": [
    "## 5. Save Checkpoint to Drive\n",
    "\n",
    "Prevent losing your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63515cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy training results to Google Drive (prevent losing trained model!)\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Find the checkpoint directory\n",
    "checkpoint_base = 'checkpoints'\n",
    "results_pattern = f'{checkpoint_base}/*/peru_rainfall_timerxl*/'\n",
    "\n",
    "matching_dirs = glob.glob(results_pattern)\n",
    "\n",
    "if matching_dirs:\n",
    "    results_path = matching_dirs[0]\n",
    "    \n",
    "    # Copy entire results folder to Drive\n",
    "    drive_results = '/content/drive/MyDrive/timer_xl_peru/results/'\n",
    "    os.makedirs(drive_results, exist_ok=True)\n",
    "    \n",
    "    print(\"💾 Copying results to Google Drive...\")\n",
    "    print(f\"   From: {results_path}\")\n",
    "    print(f\"   To: {drive_results}\")\n",
    "    \n",
    "    # Use shutil for better error handling\n",
    "    try:\n",
    "        shutil.copytree(results_path, os.path.join(drive_results, os.path.basename(results_path.rstrip('/'))), dirs_exist_ok=True)\n",
    "        print(\"✅ Checkpoint and results saved to Google Drive!\")\n",
    "        print(f\"📁 Location: {drive_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error copying to Drive: {e}\")\n",
    "        print(\"   You can manually copy from:\", results_path)\n",
    "else:\n",
    "    print(\"⚠️ No results found. Training may have failed or is still in progress.\")\n",
    "    print(\"   Expected pattern:\", results_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370af7c",
   "metadata": {},
   "source": [
    "## 6. Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display test results\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Find results directory\n",
    "checkpoint_base = 'checkpoints'\n",
    "results_pattern = f'{checkpoint_base}/*/peru_rainfall_timerxl*/'\n",
    "matching_dirs = glob.glob(results_pattern)\n",
    "\n",
    "if matching_dirs:\n",
    "    results_dir = matching_dirs[0]\n",
    "    print(f\"📂 Results directory: {results_dir}\\n\")\n",
    "    \n",
    "    # List all files\n",
    "    print(\"📄 Files in results:\")\n",
    "    for file in os.listdir(results_dir):\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # Try to load metrics\n",
    "    metrics_files = glob.glob(os.path.join(results_dir, '*metrics*.json'))\n",
    "    \n",
    "    if metrics_files:\n",
    "        print(f\"\\n📊 Loading metrics from: {metrics_files[0]}\")\n",
    "        with open(metrics_files[0]) as f:\n",
    "            metrics = json.load(f)\n",
    "        print(\"\\n✅ Test Metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "    else:\n",
    "        print(\"\\n⚠️ No metrics file found yet. Training may still be in progress.\")\n",
    "else:\n",
    "    print(\"⚠️ No results directory found. Training may have failed.\")\n",
    "    print(f\"   Expected pattern: {results_pattern}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
