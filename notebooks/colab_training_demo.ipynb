{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafb4827",
   "metadata": {},
   "source": [
    "# üåßÔ∏è Timer-XL Peru Rainfall Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb58468",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660775a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ChristianPE1/test-openltm-code.git\n",
    "%cd test-openltm-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92910c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524612e",
   "metadata": {},
   "source": [
    "## 2. Verificar Datos ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh datasets/raw_era5/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9543ac7",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python preprocessing/preprocess_era5_peru.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024 \\\n",
    "    --target_horizon 24 \\\n",
    "    --threshold 0.0001\n",
    "\n",
    "print(\"\\n‚úÖ Preproces complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for quick inspection\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('datasets/processed/peru_rainfall.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Load statistics\n",
    "with open('datasets/processed/preprocessing_stats.json') as f:\n",
    "    stats = json.load(f)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6979",
   "metadata": {},
   "source": [
    "## 4. Train Timer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy pre-trained checkpoint from Google Drive\n",
    "import os\n",
    "\n",
    "checkpoint_dir = 'checkpoints/timer_xl'\n",
    "checkpoint_path = f'{checkpoint_dir}/checkpoint.pth'\n",
    "\n",
    "\n",
    "!mkdir -p checkpoints/timer_xl/\n",
    "\n",
    "!cp '/content/drive/MyDrive/timer_xl_peru/checkpoints/checkpoint.pth' \\\n",
    "    checkpoints/timer_xl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298558a0",
   "metadata": {},
   "source": [
    "## Transfer Learning clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_timerxl_11years \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 8 \\\n",
    "  --d_model 1024 \\\n",
    "  --d_ff 2048 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.2 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 16 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --train_epochs 30 \\\n",
    "  --patience 8 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 30 \\\n",
    "  --adaptation \\\n",
    "  --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Transfer_Learning_11Years_2014_2024'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_timerxl_11years/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdd64",
   "metadata": {},
   "source": [
    "## üî¨ Smaller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be14cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python run.py \\\n",
    "  --task_name classification \\\n",
    "  --is_training 1 \\\n",
    "  --model_id peru_rainfall_small_efficient_11years \\\n",
    "  --model timer_xl_classifier \\\n",
    "  --data PeruRainfall \\\n",
    "  --root_path datasets/processed/ \\\n",
    "  --data_path peru_rainfall_cleaned.csv \\\n",
    "  --checkpoints checkpoints/ \\\n",
    "  --seq_len 1440 \\\n",
    "  --input_token_len 96 \\\n",
    "  --output_token_len 96 \\\n",
    "  --test_seq_len 1440 \\\n",
    "  --test_pred_len 2 \\\n",
    "  --e_layers 5 \\\n",
    "  --d_model 640 \\\n",
    "  --d_ff 1280 \\\n",
    "  --n_heads 8 \\\n",
    "  --dropout 0.15 \\\n",
    "  --activation relu \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 8e-5 \\\n",
    "  --train_epochs 25 \\\n",
    "  --patience 8 \\\n",
    "  --n_classes 2 \\\n",
    "  --gpu 0 \\\n",
    "  --cosine \\\n",
    "  --tmax 25 \\\n",
    "  --use_focal_loss \\\n",
    "  --loss CE \\\n",
    "  --itr 1 \\\n",
    "  --des 'Peru_Rainfall_Small_Efficient_11Years_2014_2024'\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"üìä Results saved to: checkpoints/peru_rainfall_small_improved_11years/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed72df",
   "metadata": {},
   "source": [
    "## Focal Loss + Regularizaci√≥n Optimizada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0306b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "!python run.py \\\n",
    "    --task_name classification \\\n",
    "    --is_training 1 \\\n",
    "    --model_id peru_rainfall_focal_optimized_v2 \\\n",
    "    --model timer_xl_classifier \\\n",
    "    --data PeruRainfall \\\n",
    "    --root_path datasets/processed/ \\\n",
    "    --data_path peru_rainfall_cleaned.csv \\\n",
    "    --checkpoints checkpoints/ \\\n",
    "    --seq_len 1440 \\\n",
    "    --input_token_len 96 \\\n",
    "    --output_token_len 96 \\\n",
    "    --test_seq_len 1440 \\\n",
    "    --test_pred_len 2 \\\n",
    "    --e_layers 5 \\\n",
    "    --d_model 640 \\\n",
    "    --d_ff 1280 \\\n",
    "    --n_heads 8 \\\n",
    "    --dropout 0.20 \\\n",
    "    --activation relu \\\n",
    "    --batch_size 40 \\\n",
    "    --learning_rate 6e-5 \\\n",
    "    --train_epochs 15 \\\n",
    "    --patience 4 \\\n",
    "    --n_classes 2 \\\n",
    "    --gpu 0 \\\n",
    "    --cosine \\\n",
    "    --tmax 15 \\\n",
    "    --use_focal_loss \\\n",
    "    --focal_alpha 0.70 \\\n",
    "    --focal_gamma 2.8 \\\n",
    "    --loss CE \\\n",
    "    --itr 1 \\\n",
    "    --des 'Peru_Focal_Optimized_V2_F182'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Entrenamiento completo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f763028",
   "metadata": {},
   "source": [
    "## Focal Loss Avanzado + Class Weights Din√°micos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ ESTRATEGIA 1: Focal Loss Agresivo + Regularizaci√≥n Optimizada\n",
    "# Target: F1 > 85% con Recall No Rain > 75%\n",
    "\n",
    "!python run.py \\\n",
    "    --task_name classification \\\n",
    "    --is_training 1 \\\n",
    "    --model_id peru_rainfall_focal_aggressive_v3 \\\n",
    "    --model timer_xl_classifier \\\n",
    "    --data PeruRainfall \\\n",
    "    --root_path datasets/processed/ \\\n",
    "    --data_path peru_rainfall_cleaned.csv \\\n",
    "    --checkpoints checkpoints/ \\\n",
    "    --seq_len 1440 \\\n",
    "    --input_token_len 96 \\\n",
    "    --output_token_len 96 \\\n",
    "    --test_seq_len 1440 \\\n",
    "    --test_pred_len 2 \\\n",
    "    --e_layers 5 \\\n",
    "    --d_model 640 \\\n",
    "    --d_ff 1280 \\\n",
    "    --n_heads 8 \\\n",
    "    --dropout 0.22 \\\n",
    "    --activation relu \\\n",
    "    --batch_size 40 \\\n",
    "    --learning_rate 7e-5 \\\n",
    "    --train_epochs 20 \\\n",
    "    --patience 5 \\\n",
    "    --n_classes 2 \\\n",
    "    --gpu 0 \\\n",
    "    --cosine \\\n",
    "    --tmax 20 \\\n",
    "    --use_focal_loss \\\n",
    "    --focal_alpha 0.75 \\\n",
    "    --focal_gamma 3.2 \\\n",
    "    --loss CE \\\n",
    "    --itr 1 \\\n",
    "    --des 'Peru_Focal_Aggressive_V3_Target_F185'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ESTRATEGIA 1 completada!\")\n",
    "print(\"üìä CAMBIOS vs V2:\")\n",
    "print(\"   ‚Ä¢ focal_alpha: 0.70 ‚Üí 0.75 (+7% peso clase No Rain)\")\n",
    "print(\"   ‚Ä¢ focal_gamma: 2.8 ‚Üí 3.2 (+14% penalizaci√≥n ejemplos f√°ciles)\")\n",
    "print(\"   ‚Ä¢ learning_rate: 6e-5 ‚Üí 7e-5 (+17% velocidad convergencia)\")\n",
    "print(\"   ‚Ä¢ dropout: 0.20 ‚Üí 0.22 (+10% regularizaci√≥n)\")\n",
    "print(\"   ‚Ä¢ patience: 4 ‚Üí 5 (m√°s tolerancia)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a00ce",
   "metadata": {},
   "source": [
    "## Arquitectura M√°s Profunda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è ESTRATEGIA 2: Modelo M√°s Profundo (7 layers)\n",
    "# Target: Mejor captura de patrones ENSO temporales complejos\n",
    "\n",
    "!python run.py \\\n",
    "    --task_name classification \\\n",
    "    --is_training 1 \\\n",
    "    --model_id peru_rainfall_deep_model_v1 \\\n",
    "    --model timer_xl_classifier \\\n",
    "    --data PeruRainfall \\\n",
    "    --root_path datasets/processed/ \\\n",
    "    --data_path peru_rainfall_cleaned.csv \\\n",
    "    --checkpoints checkpoints/ \\\n",
    "    --seq_len 1440 \\\n",
    "    --input_token_len 96 \\\n",
    "    --output_token_len 96 \\\n",
    "    --test_seq_len 1440 \\\n",
    "    --test_pred_len 2 \\\n",
    "    --e_layers 7 \\\n",
    "    --d_model 768 \\\n",
    "    --d_ff 1536 \\\n",
    "    --n_heads 8 \\\n",
    "    --dropout 0.18 \\\n",
    "    --activation relu \\\n",
    "    --batch_size 32 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --train_epochs 18 \\\n",
    "    --patience 5 \\\n",
    "    --n_classes 2 \\\n",
    "    --gpu 0 \\\n",
    "    --cosine \\\n",
    "    --tmax 18 \\\n",
    "    --use_focal_loss \\\n",
    "    --focal_alpha 0.72 \\\n",
    "    --focal_gamma 2.9 \\\n",
    "    --loss CE \\\n",
    "    --itr 1 \\\n",
    "    --des 'Peru_Deep_Model_7Layers_768Dim'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ESTRATEGIA 2 completada!\")\n",
    "print(\"üß† ARQUITECTURA PROFUNDA:\")\n",
    "print(\"   ‚Ä¢ Layers: 5 ‚Üí 7 (+40%)\")\n",
    "print(\"   ‚Ä¢ d_model: 640 ‚Üí 768 (+20%)\")\n",
    "print(\"   ‚Ä¢ d_ff: 1280 ‚Üí 1536 (+20%)\")\n",
    "print(\"   ‚Ä¢ Par√°metros totales: ~8M ‚Üí ~14M\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d136f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üåä REGRESI√ìN (Rainfall Forecasting)\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øQu√© es un \"buen\" resultado en Rainfall Regression?\n",
    "\n",
    "Basado en literatura (ERA5, IMERG precipitation):\n",
    "\n",
    "| M√©trica | Excelente | Bueno | Aceptable |\n",
    "|---------|-----------|-------|-----------|\n",
    "| **RMSE** | < 2.0 mm | < 3.5 mm | < 5.0 mm |\n",
    "| **MAE** | < 1.5 mm | < 2.5 mm | < 3.5 mm |\n",
    "| **R¬≤ Score** | > 0.70 | > 0.55 | > 0.40 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67776a19",
   "metadata": {},
   "source": [
    "## Preparar Datos desde Archivos .nc (DESDE CERO)\n",
    "**Script**: `preprocess_era5_regression.py` (procesa .nc ‚Üí CSV regresi√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä REGRESI√ìN v1: Transfer Learning Directo (Forecasting)\n",
    "# Usa checkpoint pre-entrenado en su tarea original (regresi√≥n)\n",
    "# ‚ö†Ô∏è USA EL CSV LIMPIO (peru_rainfall_regression_cleaned.csv)\n",
    "\n",
    "!python run.py \\\n",
    "    --task_name long_term_forecast \\\n",
    "    --is_training 1 \\\n",
    "    --model_id peru_rainfall_regression_baseline \\\n",
    "    --model timer_xl \\\n",
    "    --data PeruRainfall \\\n",
    "    --root_path datasets/processed/ \\\n",
    "    --data_path peru_rainfall_regression_cleaned.csv \\\n",
    "    --checkpoints checkpoints/ \\\n",
    "    --seq_len 1440 \\\n",
    "    --label_len 720 \\\n",
    "    --pred_len 24 \\\n",
    "    --input_token_len 96 \\\n",
    "    --output_token_len 96 \\\n",
    "    --e_layers 5 \\\n",
    "    --d_model 640 \\\n",
    "    --d_ff 1280 \\\n",
    "    --n_heads 8 \\\n",
    "    --dropout 0.15 \\\n",
    "    --activation relu \\\n",
    "    --batch_size 32 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --train_epochs 20 \\\n",
    "    --patience 6 \\\n",
    "    --gpu 0 \\\n",
    "    --cosine \\\n",
    "    --tmax 20 \\\n",
    "    --adaptation \\\n",
    "    --pretrain_model_path checkpoints/timer_xl/checkpoint.pth \\\n",
    "    --loss MSE \\\n",
    "    --itr 1 \\\n",
    "    --des 'Peru_Rainfall_Regression_24h_Forecast'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ REGRESI√ìN completada!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee6c4f",
   "metadata": {},
   "source": [
    "## Preparaci√≥n de Datos para Regresi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß PASO 1: Preparar datos de REGRESI√ìN desde archivos .nc (DESDE CERO)\n",
    "# ‚ö†Ô∏è NO usa peru_rainfall_cleaned.csv (tiene datos binarizados)\n",
    "# ‚úÖ Procesa directamente archivos .nc ‚Üí valores continuos (mm)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üåä PASO 1/2: PREPARANDO DATOS DE REGRESI√ìN DESDE ARCHIVOS .NC\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  DIFERENCIAS vs CLASIFICACI√ìN:\")\n",
    "print(\"   ‚ùå NO usa peru_rainfall_cleaned.csv (datos binarizados)\")\n",
    "print(\"   ‚úÖ Procesa directamente archivos .nc\")\n",
    "print(\"   ‚úÖ Convierte METROS ‚Üí MIL√çMETROS (√ó1000)\")\n",
    "print(\"   ‚úÖ Target: 'target_precip_24h' (continuo, NO binario)\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "!python preprocessing/preprocess_era5_regression.py \\\n",
    "    --input_dir datasets/raw_era5 \\\n",
    "    --output_dir datasets/processed \\\n",
    "    --years 2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024 \\\n",
    "    --target_horizon 24\n",
    "\n",
    "print(\"\\n‚úÖ Datos RAW de regresi√≥n generados!\")\n",
    "print(\"üìä Verificar estad√≠sticas:\")\n",
    "\n",
    "import json\n",
    "with open('datasets/processed/regression_stats.json') as f:\n",
    "    stats = json.load(f)\n",
    "    print(json.dumps(stats, indent=2))\n",
    "\n",
    "# üßπ PASO 2: Limpiar datos de REGRESI√ìN (manejo de NaN, outliers)\n",
    "# ‚ö†Ô∏è DIFERENTE a clasificaci√≥n (m√°s agresivo con NaN)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ PASO 2/2: LIMPIANDO DATOS DE REGRESI√ìN\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚ö†Ô∏è  DIFERENCIAS vs LIMPIEZA DE CLASIFICACI√ìN:\")\n",
    "print(\"   ‚úÖ Interpolaci√≥n temporal (mejor para series continuas)\")\n",
    "print(\"   ‚úÖ Detecci√≥n de outliers (valores extremos)\")\n",
    "print(\"   ‚úÖ Validaci√≥n de valores continuos (no binarios)\")\n",
    "print(\"   ‚úÖ Remoci√≥n agresiva de NaN (regresi√≥n es sensible)\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "!python preprocessing/clean_regression_data.py \\\n",
    "    --input_path datasets/processed/peru_rainfall_regression.csv \\\n",
    "    --output_path datasets/processed/peru_rainfall_regression_cleaned.csv \\\n",
    "    --max_precip 200.0\n",
    "\n",
    "print(\"\\n‚úÖ Datos de regresi√≥n LIMPIOS y listos para entrenamiento!\")\n",
    "print(\"\\nüìä VERIFICACI√ìN FINAL:\")\n",
    "\n",
    "# Verificar que los datos sean continuos (NO binarios)\n",
    "import pandas as pd\n",
    "df_clean = pd.read_csv('datasets/processed/peru_rainfall_regression_cleaned.csv')\n",
    "\n",
    "print(f\"\\nüéØ Target variable (target_precip_24h):\")\n",
    "print(f\"   Shape: {df_clean.shape}\")\n",
    "print(f\"   Unique values: {df_clean['target_precip_24h'].nunique()}\")\n",
    "print(f\"   Range: [{df_clean['target_precip_24h'].min():.3f}, {df_clean['target_precip_24h'].max():.3f}] mm\")\n",
    "print(f\"   Mean: {df_clean['target_precip_24h'].mean():.3f} mm\")\n",
    "print(f\"   NaN count: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Validaci√≥n cr√≠tica\n",
    "unique_count = df_clean['target_precip_24h'].nunique()\n",
    "if unique_count < 100:\n",
    "    print(f\"\\n‚ùå ERROR: Solo {unique_count} valores √∫nicos (datos binarios, no continuos)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ CORRECTO: {unique_count} valores √∫nicos (datos continuos)\")\n",
    "    print(\"‚úÖ Listo para entrenamiento de regresi√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc5fb6",
   "metadata": {},
   "source": [
    "## Clasificaci√≥n vs Regresi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Comparaci√≥n de Resultados: Clasificaci√≥n vs Regresi√≥n\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä COMPARACI√ìN DE ENFOQUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tabla de comparaci√≥n (debes llenar despu√©s de entrenar)\n",
    "comparison = {\n",
    "    'M√©trica': [\n",
    "        'Tiempo entrenamiento',\n",
    "        'F1-Score / RMSE',\n",
    "        'Recall / MAE',\n",
    "        'Precision / R¬≤',\n",
    "        'Interpretabilidad',\n",
    "        'Transfer Learning',\n",
    "        'Comparabilidad papers'\n",
    "    ],\n",
    "    'Clasificaci√≥n (V2)': [\n",
    "        '~1.5 horas',\n",
    "        '83.24%',\n",
    "        'Rain: 83% | No Rain: 71%',\n",
    "        'Rain: 83% | No Rain: 71%',\n",
    "        'Alta (binario)',\n",
    "        'Regular (cambio de dominio)',\n",
    "        'Dif√≠cil (pocas referencias)'\n",
    "    ],\n",
    "    'Regresi√≥n (Baseline)': [\n",
    "        '~1.8 horas',\n",
    "        'PENDING (ejecutar)',\n",
    "        'PENDING (ejecutar)',\n",
    "        'PENDING (ejecutar)',\n",
    "        'Muy alta (mm/d√≠a)',\n",
    "        'Excelente (mismo dominio)',\n",
    "        'F√°cil (benchmarks ERA5)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comp = pd.DataFrame(comparison)\n",
    "print(\"\\n\" + df_comp.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° RECOMENDACI√ìN PARA TESIS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. **Enfoque Principal: REGRESI√ìN**\n",
    "   - Mejor aprovecha pre-training de Timer-XL\n",
    "   - M√©tricas comparables con literatura (RMSE/MAE)\n",
    "   - Preserva informaci√≥n de intensidad (√∫til para ENSO extremos)\n",
    "\n",
    "2. **Enfoque Secundario: CLASIFICACI√ìN**\n",
    "   - √ötil para aplicaciones pr√°cticas (alertas tempranas)\n",
    "   - Complementa an√°lisis de regresi√≥n\n",
    "   - Puede convertir predicciones regresi√≥n ‚Üí clasificaci√≥n (umbral)\n",
    "\n",
    "3. **Estructura de Tesis**:\n",
    "   Cap√≠tulo 4: Resultados\n",
    "   - 4.1 Rainfall Forecasting (Regresi√≥n) - PRINCIPAL\n",
    "   - 4.2 Rain Detection (Clasificaci√≥n) - COMPLEMENTARIO\n",
    "   - 4.3 ENSO-aware Analysis (ambos enfoques)\n",
    "   - 4.4 Regional Analysis (ambos enfoques)\n",
    "\n",
    "‚úÖ **VENTAJA**: Dos l√≠neas de evaluaci√≥n hacen tu tesis m√°s robusta.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìå PR√ìXIMOS PASOS:\")\n",
    "print(\"   1. Ejecutar celda de preparaci√≥n de datos regresi√≥n\")\n",
    "print(\"   2. Ejecutar celda REGRESI√ìN v1\")\n",
    "print(\"   3. Comparar RMSE regresi√≥n vs F1 clasificaci√≥n\")\n",
    "print(\"   4. Elegir enfoque principal para tesis seg√∫n resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992ef2c",
   "metadata": {},
   "source": [
    "## üåä FASE 2: Validaci√≥n ENSO-aware (Core de tu Tesis)\n",
    "\n",
    "**validar**:\n",
    "1. **H1**: F1 > 0.75 en TODAS las fases (El Ni√±o, La Ni√±a, Neutral)\n",
    "2. **H2**: |F1_ElNi√±o - F1_LaNi√±a| < 0.15 (consistencia)\n",
    "3. **H3**: F1_ElNi√±o ‚â• F1_Neutral AND F1_LaNi√±a ‚â• F1_Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeedaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üåä FASE 2: VALIDACI√ìN ENSO-AWARE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Buscar mejor checkpoint de FASE 1\n",
    "checkpoint_pattern = \"checkpoints/classification_peru_rainfall_focal_rescue_v1_*/checkpoint.pth\"\n",
    "checkpoints = glob.glob(checkpoint_pattern)\n",
    "\n",
    "if not checkpoints:\n",
    "    print(\"\\n‚ùå ERROR: No se encontr√≥ checkpoint de FASE 1\")\n",
    "    print(\"   Ejecuta primero la celda de FASE 1 (Rescate del Modelo)\")\n",
    "    print(\"   Debe generar un checkpoint con F1 > 0.80\\n\")\n",
    "else:\n",
    "    checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "    CHECKPOINT_PATH = checkpoints[0]\n",
    "    CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_PATH)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Checkpoint encontrado: {CHECKPOINT_DIR}\")\n",
    "    \n",
    "    # Nota: validate_enso_phases.py requiere integraci√≥n con tu pipeline\n",
    "    # Por ahora, ejecuta el test normal y guarda predicciones\n",
    "    \n",
    "    print(\"\\n\udcca PASO 1: Generar predicciones del modelo...\")\n",
    "    print(\"   (Debes ejecutar el test y guardar predicciones con timestamps)\")\n",
    "    \n",
    "    # Ejecutar test y guardar predicciones\n",
    "    !python test_checkpoint_standalone.py \\\n",
    "        --checkpoint_path $CHECKPOINT_PATH \\\n",
    "        --save_predictions \\\n",
    "        --output_dir results/enso_validation\n",
    "    \n",
    "    print(\"\\nüìä PASO 2: Ejecutar an√°lisis ENSO-aware...\")\n",
    "    \n",
    "    # ‚ö†Ô∏è REQUIERE ADAPTACI√ìN: validate_enso_phases.py necesita acceso a predicciones\n",
    "    # Por ahora, placeholder - debes integrar con tu pipeline\n",
    "    \n",
    "    print(\"\\nüí° SIGUIENTE PASO:\")\n",
    "    print(\"   1. Revisa el archivo de predicciones generado\")\n",
    "    print(\"   2. A√±ade columna 'enso_phase' al CSV de predicciones\")\n",
    "    print(\"   3. Ejecuta: !python validate_enso_phases.py \\\\\")\n",
    "    print(\"              --data_path results/enso_validation/predictions_with_phases.csv \\\\\")\n",
    "    print(\"              --output_dir results/enso_validation\")\n",
    "    \n",
    "    print(\"\\nüìä M√âTRICAS ESPERADAS:\")\n",
    "    print(\"   ‚úÖ F1 El Ni√±o > 0.75\")\n",
    "    print(\"   ‚úÖ F1 La Ni√±a > 0.75\")\n",
    "    print(\"   ‚úÖ F1 Neutral > 0.75\")\n",
    "    print(\"   ‚úÖ |F1_ElNi√±o - F1_LaNi√±a| < 0.15\")\n",
    "    \n",
    "    print(\"\\nüìÅ Resultados se guardar√°n en: results/enso_validation/\")\n",
    "    print(\"   - enso_f1_comparison.png\")\n",
    "    print(\"   - enso_confusion_matrices.png\")\n",
    "    print(\"   - enso_validation_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263aa5a4",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è FASE 3: An√°lisis Regional (Costa Norte vs Centro vs Sur)\n",
    "\n",
    "**Objetivo**: Validar gradiente de influencia ENSO.\n",
    "\n",
    "**Hip√≥tesis a validar**:\n",
    "1. **H4**: F1_Norte > F1_Centro > F1_Sur (gradiente ENSO)\n",
    "2. **H5**: Rain_prevalence_Norte > Rain_prevalence_Sur\n",
    "\n",
    "**Requisito previo**: Haber completado FASE 1 y FASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üó∫Ô∏è FASE 3: Ejecutar An√°lisis Regional\n",
    "# ‚ö†Ô∏è REQUIERE: Datos con coordenadas geogr√°ficas (latitud, longitud)\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üó∫Ô∏è FASE 3: AN√ÅLISIS REGIONAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar que existan predicciones con coordenadas\n",
    "predictions_file = \"results/enso_validation/predictions_with_coords.csv\"\n",
    "\n",
    "if not os.path.exists(predictions_file):\n",
    "    print(\"\\n‚ö†Ô∏è NOTA: Se requiere CSV con predicciones + coordenadas\")\n",
    "    print(\"   Columnas necesarias:\")\n",
    "    print(\"   - timestamp\")\n",
    "    print(\"   - latitude (para asignar regi√≥n)\")\n",
    "    print(\"   - rain_24h (label verdadero)\")\n",
    "    print(\"   - pred_label (predicci√≥n del modelo)\")\n",
    "    print(\"   - pred_proba_rain (probabilidad clase Rain)\")\n",
    "    \n",
    "    print(\"\\nüí° CREAR CSV:\")\n",
    "    print(\"   1. Cargar datos originales (peru_rainfall_cleaned.csv)\")\n",
    "    print(\"   2. A√±adir columnas de predicci√≥n del modelo\")\n",
    "    print(\"   3. Guardar como predictions_with_coords.csv\")\n",
    "    \n",
    "    print(\"\\nüìä REGIONES (basado en latitud):\")\n",
    "    print(\"   - Costa Norte (-8¬∞ a -4¬∞): Piura, Tumbes, Lambayeque\")\n",
    "    print(\"   - Costa Centro (-14¬∞ a -8¬∞): Lima, Callao, Ica\")\n",
    "    print(\"   - Costa Sur (-18¬∞ a -14¬∞): Arequipa, Moquegua, Tacna\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Archivo de predicciones encontrado: {predictions_file}\")\n",
    "    \n",
    "    print(\"\\nüìä Ejecutando an√°lisis regional...\")\n",
    "    \n",
    "    !python validate_regional.py \\\n",
    "        --data_path $predictions_file \\\n",
    "        --output_dir results/regional_analysis\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ AN√ÅLISIS REGIONAL COMPLETADO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä VERIFICAR HIP√ìTESIS:\")\n",
    "    print(\"   ‚úÖ H4: ¬øF1_Norte > F1_Centro > F1_Sur?\")\n",
    "    print(\"   ‚úÖ H5: ¬øRain_prevalence_Norte > Rain_prevalence_Sur?\")\n",
    "    \n",
    "    print(\"\\nüìÅ Resultados guardados en: results/regional_analysis/\")\n",
    "    print(\"   - regional_comparison.png\")\n",
    "    print(\"   - regional_confusion_matrices.png\")\n",
    "    print(\"   - regional_analysis_report.txt\")\n",
    "    \n",
    "    print(\"\\nüí° INTERPRETACI√ìN:\")\n",
    "    print(\"   Si H4 se cumple ‚Üí Timer-XL captura gradiente ENSO ‚úÖ\")\n",
    "    print(\"   Si H4 NO se cumple ‚Üí Requiere features ENSO expl√≠citos ‚ö†Ô∏è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699a1c1",
   "metadata": {},
   "source": [
    "## TEST ANY CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a724c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test_checkpoint_standalone.py --find_latest\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Para testear un checkpoint espec√≠fico, usa:\")\n",
    "print(\"   !python test_checkpoint_standalone.py --checkpoint_path 'ruta/al/checkpoint.pth'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba25f5",
   "metadata": {},
   "source": [
    "## üíæ GUARDAR CHECKPOINTS ANTES DE DESCONECTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Backup autom√°tico de checkpoints a Google Drive\n",
    "# Ejecuta esta celda ANTES de desconectar Colab para guardar todo tu progreso\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üíæ GUARDANDO CHECKPOINTS A GOOGLE DRIVE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Directorio de destino en Drive\n",
    "drive_backup = '/content/drive/MyDrive/timer_xl_peru/checkpoints_backup/'\n",
    "os.makedirs(drive_backup, exist_ok=True)\n",
    "\n",
    "# Timestamp para identificar este backup\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Buscar TODOS los checkpoints generados\n",
    "checkpoint_patterns = [\n",
    "    'checkpoints/classification_peru_rainfall_timerxl_11years_*/',\n",
    "    'checkpoints/classification_peru_rainfall_small_improved_11years_*/',\n",
    "    'checkpoints/classification_peru_rainfall_timerxl_*/',\n",
    "    'checkpoints/classification_peru_rainfall_small_*/'\n",
    "]\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "for pattern in checkpoint_patterns:\n",
    "    matching_dirs = glob.glob(pattern)\n",
    "    \n",
    "    for checkpoint_dir in matching_dirs:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            # Nombre descriptivo para el backup\n",
    "            model_name = os.path.basename(checkpoint_dir.rstrip('/'))\n",
    "            backup_name = f\"{model_name}_{timestamp}.pth\"\n",
    "            backup_path = os.path.join(drive_backup, backup_name)\n",
    "            \n",
    "            # Copiar checkpoint\n",
    "            print(f\"üì¶ Guardando: {model_name}\")\n",
    "            print(f\"   Origen: {checkpoint_path}\")\n",
    "            print(f\"   Destino: {backup_path}\")\n",
    "            \n",
    "            try:\n",
    "                shutil.copy2(checkpoint_path, backup_path)\n",
    "                \n",
    "                # Obtener tama√±o del archivo\n",
    "                size_mb = os.path.getsize(backup_path) / (1024**2)\n",
    "                print(f\"   ‚úÖ Guardado exitoso ({size_mb:.1f} MB)\\n\")\n",
    "                \n",
    "                saved_models.append({\n",
    "                    'name': model_name,\n",
    "                    'path': backup_path,\n",
    "                    'size_mb': size_mb\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"=\"*80)\n",
    "print(\"üìä RESUMEN DEL BACKUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if saved_models:\n",
    "    print(f\"\\n‚úÖ {len(saved_models)} checkpoint(s) guardado(s):\\n\")\n",
    "    \n",
    "    total_size = 0\n",
    "    for model in saved_models:\n",
    "        print(f\"   ‚Ä¢ {model['name']}\")\n",
    "        print(f\"     Tama√±o: {model['size_mb']:.1f} MB\")\n",
    "        print(f\"     Ubicaci√≥n: {model['path']}\\n\")\n",
    "        total_size += model['size_mb']\n",
    "    \n",
    "    print(f\"üíæ Tama√±o total: {total_size:.1f} MB\")\n",
    "    print(f\"üìÅ Directorio: {drive_backup}\")\n",
    "    \n",
    "    # Guardar tambi√©n metadata\n",
    "    metadata_path = os.path.join(drive_backup, f'backup_metadata_{timestamp}.txt')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        f.write(f\"Backup realizado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total checkpoints: {len(saved_models)}\\n\")\n",
    "        f.write(f\"Tama√±o total: {total_size:.1f} MB\\n\\n\")\n",
    "        f.write(\"Checkpoints guardados:\\n\")\n",
    "        for model in saved_models:\n",
    "            f.write(f\"  - {model['name']} ({model['size_mb']:.1f} MB)\\n\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Metadata guardada: {metadata_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se encontraron checkpoints para guardar.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ BACKUP COMPLETADO - Ya puedes desconectar Colab de forma segura\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
